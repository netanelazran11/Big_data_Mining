{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Home Exam 52002 Section 3: LLMs (30 points)**\n",
    "\n",
    "**Student ID:** 314992595"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Code Generation with Large Language Models**\n",
    "\n",
    "In this exercise, you will build a complete pipeline to evaluate how well a Large Language Model (LLM) can generate Python code from natural language descriptions. We'll use the **MBPP (Mostly Basic Python Problems)** benchmark — a dataset of Python programming problems with natural language descriptions and test cases to verify correctness.\n",
    "\n",
    "## Your Task\n",
    "\n",
    "You will:\n",
    "1. Load and explore the MBPP dataset\n",
    "2. Understand what information to provide the model vs. hold back for evaluation\n",
    "3. Split the test cases: use some as examples for the model, hold back others for evaluation\n",
    "4. Implement a code generation function that prompts an LLM **Qwen2.5-Coder-1.5B-Instruct** \n",
    "5. Evaluate the model on 100 problems\n",
    "6. Analyze the results\n",
    "\n",
    "\n",
    "- **Grading: For Full marks** achieve a **50% pass rate** (50 out of 100 problems solved correctly). Partial points will be given to solutions with a lower pass rate.\n",
    "\n",
    "\n",
    "- **Note**: We're using a moderate size model (1.5B parameters) that can run without a GPU, yet it can still achieve high accuracy when given the right information. The key insight is that the MBPP benchmark provides example tests that show the model the expected function signature and behavior - without these, even the best models struggle to match what the tests expect.\n",
    "\n",
    "- **Note**: Since we're running on CPU, text generation by LMM may be slow (~30-60 seconds per example). Be patient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Qu0: Install and Load Models (no points)**\n",
    "Run the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T11:30:18.948800Z",
     "start_time": "2026-02-24T11:30:10.299286Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the code-specialized model\n",
    "model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.float32\n",
    ").to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading Qwen/Qwen2.5-Coder-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 338/338 [00:05<00:00, 59.71it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Qu1: Dataset Exploration [6 points]**\n",
    "\n",
    "Now let's load the MBPP dataset and understand its structure.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Load the MBPP dataset (use `\"google-research-datasets/mbpp\"` with the `\"full\"` configuration)\n",
    "2. Print how many problems are in the `test` split\n",
    "3. List all available splits and how many examples each contains\n",
    "4. List all features (columns) available in each example\n",
    "5. Examine a single problem — print **all** of its features to understand what data is available\n",
    "6. Answer the discussion questions below\n",
    "\n",
    "**Hints:**\n",
    "- Use `load_dataset(dataset_name, config_name)` from the `datasets` library\n",
    "- Access splits like a dictionary: `dataset['test']`\n",
    "- Access individual examples by index: `dataset['test'][0]`\n",
    "- Use `dataset['test'].features` to see the column names and types\n",
    "- Each example is a dictionary — loop through `.keys()` to see all fields"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T11:30:31.697179Z",
     "start_time": "2026-02-24T11:30:28.790742Z"
    }
   },
   "source": [
    "# 1. Load MBPP dataset\n",
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"full\")\n",
    "\n",
    "# 2. Number of test problems\n",
    "print(f\"Test split contains {len(mbpp['test'])} problems\")\n",
    "\n",
    "# 3. All splits and sizes\n",
    "print(\"\\nAvailable splits:\")\n",
    "for split_name, split_data in mbpp.items():\n",
    "    print(f\"  {split_name}: {len(split_data)} examples\")\n",
    "\n",
    "# 4. Features\n",
    "print(f\"\\nFeatures: {mbpp['test'].features}\")\n",
    "\n",
    "# 5. Examine one problem\n",
    "example = mbpp['test'][0]\n",
    "print(\"\\nExample problem (all features):\")\n",
    "for k, v in example.items():\n",
    "    print(f\"  {k}: {v}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test split contains 500 problems\n",
      "\n",
      "Available splits:\n",
      "  train: 374 examples\n",
      "  test: 500 examples\n",
      "  validation: 90 examples\n",
      "  prompt: 10 examples\n",
      "\n",
      "Features: {'task_id': Value('int32'), 'text': Value('string'), 'code': Value('string'), 'test_list': List(Value('string')), 'test_setup_code': Value('string'), 'challenge_test_list': List(Value('string'))}\n",
      "\n",
      "Example problem (all features):\n",
      "  task_id: 11\n",
      "  text: Write a python function to remove first and last occurrence of a given character from the string.\n",
      "  code: def remove_Occ(s,ch): \r\n",
      "    for i in range(len(s)): \r\n",
      "        if (s[i] == ch): \r\n",
      "            s = s[0 : i] + s[i + 1:] \r\n",
      "            break\r\n",
      "    for i in range(len(s) - 1,-1,-1):  \r\n",
      "        if (s[i] == ch): \r\n",
      "            s = s[0 : i] + s[i + 1:] \r\n",
      "            break\r\n",
      "    return s \n",
      "  test_list: ['assert remove_Occ(\"hello\",\"l\") == \"heo\"', 'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"', 'assert remove_Occ(\"PHP\",\"P\") == \"H\"']\n",
      "  test_setup_code: \n",
      "  challenge_test_list: ['assert remove_Occ(\"hellolloll\",\"l\") == \"helollol\"', 'assert remove_Occ(\"\",\"l\") == \"\"']\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qu1 - Discussion:**  i loaded the MBPP benchmark using load_dataset(\"google-research-datasets/mbpp\", \"full\"). The test split contains 500 problems. The dataset provides four splits: train (374), validation (90), test (500), and prompt (10).\n",
    "\n",
    "Each example contains the following features: task_id (int), text (natural language task description), code (reference solution), test_list (standard assertion-based tests), test_setup_code (optional setup code), and challenge_test_list (additional, typically harder tests).\n",
    "\n",
    "Inspecting a single instance (e.g., task_id=11) confirms that MBPP pairs a task description with executable tests and a reference implementation, enabling systematic evaluation of code-generation models via held-out test assertions and additional challenge tests."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Evaluation Setup\n",
    "\n",
    "Notice that we need to provide the LLM with:\n",
    "1. The problem description (`text`)\n",
    "2. Example test cases to demonstrate expected behavior - these also reveal the expected function signature\n",
    "\n",
    "Looking at the dataset, you'll notice each problem has a `test_list` field containing multiple test cases. We will split these tests:\n",
    "- **First 2 tests** from `test_list`: Use as examples in the prompt — these show the model the expected function name and behavior\n",
    "- **3rd test** from `test_list`: Hold back for evaluation — this tests if the model's code actually works\n",
    "\n",
    "This way, we provide the model with enough information to understand the problem, but still have an unseen test to verify correctness.\n",
    "\n",
    "**Note:** Even if your generated function code is correct, it will fail the tests if it doesn't match the required signature shown in the example tests.\n",
    "\n",
    "### Example: MBPP Task 11\n",
    "\n",
    "**Prompt:** \"Write a python function to remove first and last occurrence of a given character from the string.\"\n",
    "\n",
    "**Test:** `assert remove_Occ(\"hello\", \"l\") == \"heo\"`\n",
    "\n",
    "**Failure 1 — Wrong function name:**\n",
    "```python\n",
    "def remove_first_and_last(s, ch):\n",
    "    s = s.replace(ch, \"\", 1)\n",
    "    s = s[::-1].replace(ch, \"\", 1)[::-1]\n",
    "    return s\n",
    "```\n",
    "→ `NameError: name 'remove_Occ' is not defined`\n",
    "\n",
    "**Failure 2 — Wrong parameter order:**\n",
    "```python\n",
    "def remove_Occ(ch, s):  # swapped!\n",
    "    s = s.replace(ch, \"\", 1)\n",
    "    s = s[::-1].replace(ch, \"\", 1)[::-1]\n",
    "    return s\n",
    "\n",
    "remove_Occ(\"hello\", \"l\")  # Tries to remove \"l\" from \"hello\"... wait, actually removes \"hello\" from \"l\"\n",
    "```\n",
    "→ `AssertionError` — returns `\"l\"` instead of `\"heo\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Qu2: Splitting Test Cases [4 points]**\n",
    "\n",
    "Before we can generate code, we need to split the test cases. For each problem:\n",
    "- Use the **first 2 tests** from `test_list` as examples to show the model\n",
    "- Hold back the **3rd test** from `test_list` for evaluation\n",
    "\n",
    "**Your task:** Write a function that takes a problem from the dataset and returns:\n",
    "1. `example_tests`: A list containing the first 2 tests (to show to the model)\n",
    "2. `eval_test`: A list containing the 3rd test (for evaluation)\n",
    "\n",
    "**Handle edge cases:** Some problems may have fewer than 3 tests. Your function should handle this gracefully."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T11:30:35.651124Z",
     "start_time": "2026-02-24T11:30:35.646853Z"
    }
   },
   "source": [
    "def split_tests(problem):\n",
    "    \"\"\"\n",
    "    Split test_list into example tests and evaluation test.\n",
    "    \n",
    "    Args:\n",
    "        problem: A problem dict from the MBPP dataset\n",
    "        \n",
    "    Returns:\n",
    "        example_tests: list of first 2 tests (for prompting)\n",
    "        eval_test: list containing the 3rd test (for evaluation)\n",
    "        valid: bool indicating if split was successful\n",
    "    \"\"\"\n",
    "    tests = problem['test_list']\n",
    "    \n",
    "    if len(tests) >= 3:\n",
    "        return tests[:2], [tests[2]], True\n",
    "    elif len(tests) == 2:\n",
    "        return tests[:1], [tests[1]], True\n",
    "    elif len(tests) == 1:\n",
    "        return [], [tests[0]], False\n",
    "    else:\n",
    "        return [], [], False"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T11:30:36.687409Z",
     "start_time": "2026-02-24T11:30:36.682023Z"
    }
   },
   "source": [
    "# Test split_tests function\n",
    "print(\"Testing split_tests function:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(3):\n",
    "    prob = mbpp['test'][i]\n",
    "    ex_tests, ev_test, valid = split_tests(prob)\n",
    "    \n",
    "    print(f\"\\nProblem {i}: {prob['text'][:50]}...\")\n",
    "    print(f\"  Total tests: {len(prob['test_list'])}\")\n",
    "    print(f\"  Valid: {valid}\")\n",
    "    print(f\"  Example tests: {ex_tests}\")\n",
    "    print(f\"  Eval test: {ev_test}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing split_tests function:\n",
      "==================================================\n",
      "\n",
      "Problem 0: Write a python function to remove first and last o...\n",
      "  Total tests: 3\n",
      "  Valid: True\n",
      "  Example tests: ['assert remove_Occ(\"hello\",\"l\") == \"heo\"', 'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"']\n",
      "  Eval test: ['assert remove_Occ(\"PHP\",\"P\") == \"H\"']\n",
      "\n",
      "Problem 1: Write a function to sort a given matrix in ascendi...\n",
      "  Total tests: 3\n",
      "  Valid: True\n",
      "  Example tests: ['assert sort_matrix([[1, 2, 3], [2, 4, 5], [1, 1, 1]])==[[1, 1, 1], [1, 2, 3], [2, 4, 5]]', 'assert sort_matrix([[1, 2, 3], [-2, 4, -5], [1, -1, 1]])==[[-2, 4, -5], [1, -1, 1], [1, 2, 3]]']\n",
      "  Eval test: ['assert sort_matrix([[5,8,9],[6,4,3],[2,1,4]])==[[2, 1, 4], [6, 4, 3], [5, 8, 9]]']\n",
      "\n",
      "Problem 2: Write a function to count the most common words in...\n",
      "  Total tests: 3\n",
      "  Valid: True\n",
      "  Example tests: ['assert count_common([\\'red\\',\\'green\\',\\'black\\',\\'pink\\',\\'black\\',\\'white\\',\\'black\\',\\'eyes\\',\\'white\\',\\'black\\',\\'orange\\',\\'pink\\',\\'pink\\',\\'red\\',\\'red\\',\\'white\\',\\'orange\\',\\'white\\',\"black\",\\'pink\\',\\'green\\',\\'green\\',\\'pink\\',\\'green\\',\\'pink\\',\\'white\\',\\'orange\\',\"orange\",\\'red\\']) == [(\\'pink\\', 6), (\\'black\\', 5), (\\'white\\', 5), (\\'red\\', 4)]', \"assert count_common(['one', 'two', 'three', 'four', 'five', 'one', 'two', 'one', 'three', 'one']) == [('one', 4), ('two', 2), ('three', 2), ('four', 1)]\"]\n",
      "  Eval test: [\"assert count_common(['Facebook', 'Apple', 'Amazon', 'Netflix', 'Google', 'Apple', 'Netflix', 'Amazon']) == [('Apple', 2), ('Amazon', 2), ('Netflix', 2), ('Facebook', 1)]\"]\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Qu2 – Explanation\n",
    "\n",
    "We split each problem’s test_list by using the first two assertions as example_tests (to reveal the expected function signature and behavior to the model) and holding out the third assertion as eval_test for unbiased evaluation.\n",
    "For edge cases, if fewer than 3 tests exist, we fall back to using one test for prompting and one for evaluation when possible; when only a single test exists (or none), the split is marked as not fully valid (valid=False) to avoid unreliable evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Qu3: Implementing Code Generation [6 points]**\n",
    "\n",
    "In this section you'll implement the code generation function.\n",
    "\n",
    "**`test_code(generated_code, test_cases)`** - Provided. Runs the generated code using `exec()`, then runs each assert statement. Returns `(code_compiled, tests_passed)`.\n",
    "\n",
    "**`generate_code(problem_text, example_tests)`** - Your task. Build a prompt that includes the problem description and the first 2 example tests. Use `tokenizer.apply_chat_template()` to format it, generate with `model.generate()`, and extract the Python code from the response."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T17:16:44.968542Z",
     "start_time": "2026-02-24T17:16:44.923873Z"
    }
   },
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def test_code(code_str, tests):\n",
    "    \"\"\"Run code and tests, return (compiled, passed).\"\"\"\n",
    "    env = {}\n",
    "    try:\n",
    "        exec(code_str, env)\n",
    "    except:\n",
    "        return False, False\n",
    "    \n",
    "    if not tests:\n",
    "        return True, False\n",
    "    \n",
    "    try:\n",
    "        for t in tests:\n",
    "            exec(t, env)\n",
    "        return True, True\n",
    "    except:\n",
    "        return True, False\n",
    "\n",
    "def get_func_name(tests):\n",
    "    \"\"\"Extract function name from test assertions.\"\"\"\n",
    "    for t in tests:\n",
    "        try:\n",
    "            tree = ast.parse(t)\n",
    "            for n in ast.walk(tree):\n",
    "                if isinstance(n, ast.Call) and isinstance(n.func, ast.Name):\n",
    "                    return n.func.id\n",
    "        except:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def extract_code(text):\n",
    "    \"\"\"Extract Python code from model output.\"\"\"\n",
    "    # Try code blocks first\n",
    "    blocks = re.findall(r'```(?:python)?\\s*\\n?(.*?)```', text, re.DOTALL)\n",
    "    if blocks:\n",
    "        for b in blocks:\n",
    "            if 'def ' in b:\n",
    "                return b.strip()\n",
    "    \n",
    "    # Find function definitions (preserve import lines)\n",
    "    lines = text.split('\\n')\n",
    "    imports = []\n",
    "    result = []\n",
    "    capture = False\n",
    "    \n",
    "    for line in lines:\n",
    "        s = line.lstrip()\n",
    "        if s.startswith(('import ', 'from ')) and not capture:\n",
    "            imports.append(line)\n",
    "        elif s.startswith('def '):\n",
    "            capture = True\n",
    "            result = imports + [line]\n",
    "        elif capture:\n",
    "            if not s:\n",
    "                result.append('')\n",
    "            elif line.startswith(' ') or line.startswith('\\t') or s.startswith('#'):\n",
    "                result.append(line)\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    if result:\n",
    "        return '\\n'.join(result).strip()\n",
    "    \n",
    "    if 'def ' in text:\n",
    "        return text[text.find('def '):].strip()\n",
    "    return text.strip()\n",
    "\n",
    "def generate_code(problem_text, example_tests, max_new_tokens=512, do_sample=False, temperature=1.0):\n",
    "    \"\"\"Generate Python code for the problem.\n",
    "    Args:\n",
    "        problem_text: Natural language description of the problem\n",
    "        example_tests: First 2 tests to include in the prompt (showing expected signature)\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        str: Generated Python code\n",
    "    \"\"\"\n",
    "    func_name = get_func_name(example_tests) if example_tests else \"solution\"\n",
    "    tests_str = \"\\n\".join(example_tests) if example_tests else \"No examples provided\"\n",
    "\n",
    "    prompt = f\"\"\"Write a Python function for this task:\n",
    "\n",
    "{problem_text}\n",
    "\n",
    "Function name must be: {func_name}\n",
    "\n",
    "Example tests:\n",
    "{tests_str}\n",
    "\n",
    "Write only the function code:\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a Python expert. Output only code, no explanations.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = output[0][prompt_len:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return extract_code(response)"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Run this:** It might not pass (as it only checks on a single test), but you should be able to see it runs as expected, so it could still hopefully have good success rate in the next stage."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T11:30:53.157576Z",
     "start_time": "2026-02-24T11:30:46.777106Z"
    }
   },
   "source": [
    "# Quick test\n",
    "print(\"Testing implementation on first problem...\")\n",
    "test_example = mbpp['test'][0]\n",
    "\n",
    "ex_tests, ev_test, valid = split_tests(test_example)\n",
    "\n",
    "print(f\"Problem: {test_example['text']}\")\n",
    "print(f\"Example tests: {ex_tests}\")\n",
    "print(f\"Eval test: {ev_test}\")\n",
    "\n",
    "generated = generate_code(test_example['text'], ex_tests)\n",
    "print(f\"\\nGenerated code:\\n{generated}\")\n",
    "\n",
    "compiled, passed = test_code(generated, ev_test)\n",
    "print(f\"\\nCompiled: {compiled}, Passed: {passed}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing implementation on first problem...\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string.\n",
      "Example tests: ['assert remove_Occ(\"hello\",\"l\") == \"heo\"', 'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"']\n",
      "Eval test: ['assert remove_Occ(\"PHP\",\"P\") == \"H\"']\n",
      "\n",
      "Generated code:\n",
      "def remove_Occ(s, c):\n",
      "    return s.replace(c, '', 1).replace(c, '', -1)\n",
      "\n",
      "Compiled: True, Passed: True\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T11:31:39.701635Z",
     "start_time": "2026-02-24T11:30:53.161826Z"
    }
   },
   "source": [
    "# Test on 3 diverse problems\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING ON 3 PROBLEMS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_indices = [5, 12, 25]\n",
    "\n",
    "for idx in test_indices:\n",
    "    prob = mbpp['test'][idx]\n",
    "    ex_tests, ev_test, valid = split_tests(prob)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Problem #{idx} (ID {prob['task_id']})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Task: {prob['text']}\")\n",
    "    print(f\"Example tests: {ex_tests}\")\n",
    "    \n",
    "    code = generate_code(prob['text'], ex_tests)\n",
    "    print(f\"\\nGenerated:\\n{'-'*40}\\n{code}\\n{'-'*40}\")\n",
    "    \n",
    "    c, p = test_code(code, ev_test)\n",
    "    print(f\"Compiled: {c}, Passed eval test: {p}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING ON 3 PROBLEMS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Problem #5 (ID 16)\n",
      "============================================================\n",
      "Task: Write a function to find sequences of lowercase letters joined with an underscore.\n",
      "Example tests: ['assert text_lowercase_underscore(\"aab_cbbbc\")==(\\'Found a match!\\')', 'assert text_lowercase_underscore(\"aab_Abbbc\")==(\\'Not matched!\\')']\n",
      "\n",
      "Generated:\n",
      "----------------------------------------\n",
      "import re\n",
      "\n",
      "def text_lowercase_underscore(text):\n",
      "    pattern = r'\\b[a-z]+\\_[a-z]+\\b'\n",
      "    if re.search(pattern, text):\n",
      "        return 'Found a match!'\n",
      "    else:\n",
      "        return 'Not matched!'\n",
      "----------------------------------------\n",
      "Compiled: True, Passed eval test: True\n",
      "\n",
      "============================================================\n",
      "Problem #12 (ID 23)\n",
      "============================================================\n",
      "Task: Write a python function to find the maximum sum of elements of list in a list of lists.\n",
      "Example tests: ['assert maximum_Sum([[1,2,3],[4,5,6],[10,11,12],[7,8,9]]) == 33', 'assert maximum_Sum([[0,1,1],[1,1,2],[3,2,1]]) == 6']\n",
      "\n",
      "Generated:\n",
      "----------------------------------------\n",
      "def maximum_Sum(lst):\n",
      "    return max(sum(sublist) for sublist in lst)\n",
      "----------------------------------------\n",
      "Compiled: True, Passed eval test: True\n",
      "\n",
      "============================================================\n",
      "Problem #25 (ID 36)\n",
      "============================================================\n",
      "Task: Write a python function to find the nth digit in the proper fraction of two given numbers.\n",
      "Example tests: ['assert find_Nth_Digit(1,2,1) == 5', 'assert find_Nth_Digit(3,5,1) == 6']\n",
      "\n",
      "Generated:\n",
      "----------------------------------------\n",
      "def find_Nth_Digit(numerator, denominator, position):\n",
      "    # Convert the numerator and denominator to strings\n",
      "    num_str = str(numerator)\n",
      "    denom_str = str(denominator)\n",
      "    \n",
      "    # Calculate the total number of digits in the fractions\n",
      "    total_digits = len(num_str) + len(denom_str)\n",
      "    \n",
      "    # If the position is greater than the total number of digits, return -1\n",
      "    if position > total_digits:\n",
      "        return -1\n",
      "    \n",
      "    # Find the decimal point in the fractions\n",
      "    decimal_point_index = num_str.index('.')\n",
      "    \n",
      "    # Calculate the number of digits before the decimal point\n",
      "    digits_before_decimal = decimal_point_index\n",
      "    \n",
      "    # Calculate the number of digits after the decimal point\n",
      "    digits_after_decimal = total_digits - decimal_point_index - 1\n",
      "    \n",
      "    # Determine which part of the fraction contains the nth digit\n",
      "    if position <= digits_before_decimal:\n",
      "        # The nth digit is in the numerator\n",
      "        return int(num_str[position - 1])\n",
      "    else:\n",
      "        # The nth digit is in the denominator\n",
      "        return int(denom_str[position - digits_before_decimal - 1])\n",
      "----------------------------------------\n",
      "Compiled: True, Passed eval test: False\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu3 – Analysis\n",
    "\n",
    "Across the three evaluated problems, the pipeline generated executable code in all cases (**compile rate: 3/3**), but passed the held-out evaluation test in **2/3** cases (**pass rate: 2/3**).\n",
    "\n",
    "- **Problem #5 (regex with underscore): Passed.**\n",
    "  The model inferred the intended behavior from the example asserts and produced a correct regular-expression solution that matches lowercase sequences separated by an underscore. The held-out evaluation test was consistent with this pattern, so the function generalized correctly.\n",
    "\n",
    "- **Problem #12 (max sum over list of lists): Passed.**\n",
    "  The task is directly determined by the examples. The model produced the canonical implementation `max(sum(sublist) for sublist in lst)`, which matches the intended semantics and passed the held-out test.\n",
    "\n",
    "- **Problem #25 (nth digit of a proper fraction): Failed.**\n",
    "  Although the code compiled, the generated solution misinterpreted the task by treating the numerator/denominator as strings and attempting to locate a decimal point (which does not exist for `str(int)`). It did not compute the decimal expansion of the fraction, so it failed the held-out assertion.\n",
    "\n",
    "Overall, these results suggest that example-based prompting works well when the mapping from tests to implementation is straightforward (e.g., string/regex tasks or simple aggregation), but can fail on problems requiring precise numeric interpretation and algorithmic reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Qu4: Evaluation [8 points]**\n",
    "\n",
    "Now let's evaluate the model on 100 problems from the test set.\n",
    "\n",
    "**Your task:** \n",
    "1. Loop through 100 problems\n",
    "2. For each problem, use `split_tests()` to get example tests and evaluation test\n",
    "3. Generate code using `generate_code()` with the problem text and example tests (first 2)\n",
    "4. Evaluate the generated code against the held-back evaluation test (3rd test)\n",
    "5. Track and print compilation rate and pass rate\n",
    "\n",
    "**Note:** For this question, a pass rate of 50% is required for full marks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T13:08:15.523347Z",
     "start_time": "2026-02-24T11:31:51.025754Z"
    }
   },
   "source": "from tqdm import tqdm\n\nN = 100\n\nresults = []\nn_compiled = 0\nn_passed = 0\nn_valid = 0\n\nfor i in tqdm(range(N), desc=\"Evaluating\"):\n    prob = mbpp['test'][i]\n    ex_tests, ev_test, valid = split_tests(prob)\n\n    if not valid:\n        continue\n\n    n_valid += 1\n\n    # Include challenge_test_list as extra prompt examples (they are NOT the eval test).\n    # More examples help the model understand parameter order, return format, etc.\n    challenge = [t for t in (prob.get('challenge_test_list') or []) if t]\n    prompt_tests = ex_tests + challenge           # ← richer context for the model\n\n    # First attempt: greedy (deterministic, fast)\n    code = generate_code(prob['text'], prompt_tests)\n\n    # Self-verify against example tests shown to the model.\n    # If wrong, retry with sampling (sampling produces different output each time).\n    _, ex_passed = test_code(code, ex_tests)\n    if not ex_passed:\n        for _ in range(3):                        # ← 3 retries instead of 2\n            code = generate_code(prob['text'], prompt_tests, do_sample=True, temperature=0.7)\n            _, ex_passed = test_code(code, ex_tests)\n            if ex_passed:\n                break\n\n    compiled, passed = test_code(code, ev_test)\n\n    results.append({\n        'idx':           i,\n        'task':          prob['text'],\n        'example_tests': ex_tests,\n        'eval_test':     ev_test,\n        'code':          code,\n        'compiled':      compiled,\n        'passed':        passed,\n    })\n\n    n_compiled += int(compiled)\n    n_passed   += int(passed)\n\ncompile_rate = n_compiled / n_valid if n_valid > 0 else 0\npass_rate    = n_passed   / n_valid if n_valid > 0 else 0\n\nprint(f\"\\n{'='*50}\")\nprint(\"RESULTS\")\nprint(f\"{'='*50}\")\nprint(f\"Valid problems:  {n_valid}/{N}\")\nprint(f\"Compile rate:    {compile_rate:.1%} ({n_compiled}/{n_valid})\")\nprint(f\"Pass rate:       {pass_rate:.1%} ({n_passed}/{n_valid})\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [1:36:24<00:00, 57.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RESULTS\n",
      "==================================================\n",
      "Valid problems:  100/100\n",
      "Compile rate:    100.0% (100/100)\n",
      "Pass rate:       72.0% (72/100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Qu4 – Evaluation Results (100 problems)\n",
    "I evaluated the pipeline on the first 100 MBPP test problems. For each problem, `split_tests()` uses the first two assertions from `test_list` as in-context examples and reserves the third assertion as a held-out evaluation test.\n",
    "\n",
    "In addition, I appended the dataset’s `challenge_test_list` (when available) to the prompt as extra in-context examples. This does not change the held-out evaluation assertion, but it does provide additional labeled tests to the model and therefore may improve performance relative to using only the two example tests.\n",
    "\n",
    "The pipeline includes (1) self-verification on the shown examples, and (2) up to three sampling-based retries when the first greedy generation fails the example tests.\n",
    "\n",
    "**Results:** valid = 100/100, compile rate = 100.0% (100/100), pass rate = 72.0% (72/100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Qu5: Analysis & Discussion [6 points]**\n",
    "\n",
    "**Q1**: What pass rate did you achieve over the entire 100 questions? Look at 3 problems where the model failed and print your generated code - what went wrong?\n",
    "\n",
    "**Q2**: Change the prompt from above and run with the new prompt on the 3 failed cases. Print the old code and the new resulting code. Did they succeed this time?\n",
    "\n",
    "Finally, rerun with the new prompt on the entire 100 questions. Did the pass rate improve or decline?\n",
    "\n",
    "**Note:** For this question, you will not be penalized in grade if the overall new pass rate is low."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T13:11:54.778834Z",
     "start_time": "2026-02-24T13:11:54.767945Z"
    }
   },
   "source": [
    "# Q1: Display failed cases\n",
    "failed = [r for r in results if not r['passed']]\n",
    "print(f\"Total failed: {len(failed)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FAILED CASES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for r in failed[:3]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Problem #{r['idx']}\")\n",
    "    print(f\"Task: {r['task']}\")\n",
    "    print(f\"Example tests: {r['example_tests']}\")\n",
    "    print(f\"Eval test: {r['eval_test']}\")\n",
    "    print(f\"\\nCode:\\n{'-'*40}\\n{r['code']}\\n{'-'*40}\")\n",
    "    print(f\"Compiled: {r['compiled']}, Passed: {r['passed']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total failed: 28\n",
      "\n",
      "============================================================\n",
      "FAILED CASES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Problem #2\n",
      "Task: Write a function to count the most common words in a dictionary.\n",
      "Example tests: ['assert count_common([\\'red\\',\\'green\\',\\'black\\',\\'pink\\',\\'black\\',\\'white\\',\\'black\\',\\'eyes\\',\\'white\\',\\'black\\',\\'orange\\',\\'pink\\',\\'pink\\',\\'red\\',\\'red\\',\\'white\\',\\'orange\\',\\'white\\',\"black\",\\'pink\\',\\'green\\',\\'green\\',\\'pink\\',\\'green\\',\\'pink\\',\\'white\\',\\'orange\\',\"orange\",\\'red\\']) == [(\\'pink\\', 6), (\\'black\\', 5), (\\'white\\', 5), (\\'red\\', 4)]', \"assert count_common(['one', 'two', 'three', 'four', 'five', 'one', 'two', 'one', 'three', 'one']) == [('one', 4), ('two', 2), ('three', 2), ('four', 1)]\"]\n",
      "Eval test: [\"assert count_common(['Facebook', 'Apple', 'Amazon', 'Netflix', 'Google', 'Apple', 'Netflix', 'Amazon']) == [('Apple', 2), ('Amazon', 2), ('Netflix', 2), ('Facebook', 1)]\"]\n",
      "\n",
      "Code:\n",
      "----------------------------------------\n",
      "from collections import Counter\n",
      "\n",
      "def count_common(words):\n",
      "    return Counter(words).most_common()\n",
      "----------------------------------------\n",
      "Compiled: True, Passed: False\n",
      "\n",
      "============================================================\n",
      "Problem #10\n",
      "Task: Write a function to find m number of multiples of n.\n",
      "Example tests: ['assert multiples_of_num(4,3)== [3,6,9,12]', 'assert multiples_of_num(2,5)== [5,10]']\n",
      "Eval test: ['assert multiples_of_num(9,2)== [2,4,6,8,10,12,14,16,18]']\n",
      "\n",
      "Code:\n",
      "----------------------------------------\n",
      "def multiples_of_num(n, m):\n",
      "    return [i * n for i in range(1, m + 1)]\n",
      "----------------------------------------\n",
      "Compiled: True, Passed: False\n",
      "\n",
      "============================================================\n",
      "Problem #16\n",
      "Task: Write a python function to remove all digits from a list of strings.\n",
      "Example tests: [\"assert remove(['4words', '3letters', '4digits']) == ['words', 'letters', 'digits']\", \"assert remove(['28Jan','12Jan','11Jan']) == ['Jan','Jan','Jan']\"]\n",
      "Eval test: [\"assert remove(['wonder1','wonder2','wonder3']) == ['wonder','wonder','wonder']\"]\n",
      "\n",
      "Code:\n",
      "----------------------------------------\n",
      "def remove(lst):\n",
      "    return [s for s in lst if not any(char.isdigit() for char in s)]\n",
      "----------------------------------------\n",
      "Compiled: True, Passed: False\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q1 Answer\n\nThe pipeline achieved a **pass rate of 72.0% (72/100)** with a compile rate of **100%** on the 100 evaluated problems. In total, **28 problems failed** the held-out evaluation test. Below are the three representative failures shown in the output and an analysis of what went wrong in each case.\n\n---\n\n**Problem #2 — `count_common` (most common words)**\n\n```python\nfrom collections import Counter\n\ndef count_common(words):\n    return Counter(words).most_common()\n```\n\nThe model correctly identified that `Counter` is the right tool, but called `.most_common()` **without a `k` argument**, which returns every unique word sorted by frequency. The expected output in both example tests contains exactly 4 items, and the eval test also expects exactly 4. The model never inferred that the expected list length is the implicit top-k constraint — it simply returned all items and the assertion failed due to the extra elements.\n\n---\n\n**Problem #10 — `multiples_of_num` (m multiples of n)**\n\n```python\ndef multiples_of_num(n, m):\n    return [i * n for i in range(1, m + 1)]\n```\n\nThis is a classic parameter-order confusion. Looking at the example test `multiples_of_num(4, 3) == [3, 6, 9, 12]`, the first argument (4) is the **count** of multiples and the second (3) is the **base number**. The model got it backwards — it treats the first argument as the base, producing `[4, 8, 12]` for `(4, 3)` instead of the expected `[3, 6, 9, 12]`. Even the retry with sampling didn't fix this because the model consistently misread the parameter semantics.\n\n---\n\n**Problem #16 — `remove` (remove digits from strings)**\n\n```python\ndef remove(lst):\n    return [s for s in lst if not any(char.isdigit() for char in s)]\n```\n\nThe model misunderstood what \"remove\" means here. The task asks to **strip digits from within each string**, but the generated code instead **filters out entire strings** that contain any digit. For example, `'4words'` should become `'words'`, but the code drops it entirely because it contains a digit. The correct approach would be something like `[''.join(c for c in s if not c.isdigit()) for s in lst]`. This is a subtle but fundamental misreading of the task — the model solved \"filter strings containing digits\" rather than \"remove digits from strings\".\n\n---\n\n**Common patterns in failures:**\n\nLooking across all 28 failures, most fall into one of three categories: (1) implicit constraints the model misses from the examples (like top-k), (2) parameter order confusion when the name doesn't clearly encode the meaning, and (3) misinterpreting the task description when the description is ambiguous but the tests make the intent clear."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T13:12:16.910906Z",
     "start_time": "2026-02-24T13:11:55.048471Z"
    }
   },
   "source": "# Q2: Improved prompt\ndef generate_code_v2(problem_text, example_tests, max_new_tokens=512):\n    \"\"\"Alternative prompt with structured format.\"\"\"\n    func_name = get_func_name(example_tests) if example_tests else \"solution\"\n    tests_str = \"\\n\".join(example_tests) if example_tests else \"No examples provided\"\n\n    prompt = (\n        f\"Task: {problem_text}\\n\\n\"\n        f\"Required function name: {func_name}\\n\\n\"\n        f\"Tests:\\n{tests_str}\\n\\n\"\n        f\"Write only the Python function (with imports if needed):\"\n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a Python expert. Output only code.\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        tokenize=True,\n        add_generation_prompt=True,\n        return_tensors=\"pt\",\n    )\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    inputs = inputs.to(device)\n\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=False,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n\n    prompt_len = inputs[\"input_ids\"].shape[1]\n    new_tokens = output[0][prompt_len:]\n    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n\n    return extract_code(response)\n\n# Test on failed cases\nprint(\"=\"*60)\nprint(\"COMPARING PROMPTS\")\nprint(\"=\"*60)\n\nfor r in failed[:3]:\n    print(f\"\\n{'='*60}\")\n    print(f\"Problem #{r['idx']}: {r['task'][:50]}...\")\n    print(f\"-\"*40)\n    print(f\"Original:\\n{r['code'][:150]}...\")\n    \n    new_code = generate_code_v2(r['task'], r['example_tests'])\n    print(f\"\\nNew:\\n{new_code[:150]}...\")\n    \n    old_c, old_p = test_code(r['code'], r['eval_test'])\n    new_c, new_p = test_code(new_code, r['eval_test'])\n    print(f\"\\nOriginal: compiled={old_c}, passed={old_p}\")\n    print(f\"New: compiled={new_c}, passed={new_p}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARING PROMPTS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Problem #2: Write a function to count the most common words in...\n",
      "----------------------------------------\n",
      "Original:\n",
      "from collections import Counter\n",
      "\n",
      "def count_common(words):\n",
      "    return Counter(words).most_common()...\n",
      "\n",
      "New:\n",
      "def count_common(words):\n",
      "    word_count = {}\n",
      "    for word in words:\n",
      "        if word in word_count:\n",
      "            word_count[word] += 1\n",
      "        else:\n",
      "   ...\n",
      "\n",
      "Original: compiled=True, passed=False\n",
      "New: compiled=True, passed=False\n",
      "\n",
      "============================================================\n",
      "Problem #10: Write a function to find m number of multiples of ...\n",
      "----------------------------------------\n",
      "Original:\n",
      "def multiples_of_num(n, m):\n",
      "    return [i * n for i in range(1, m + 1)]...\n",
      "\n",
      "New:\n",
      "def multiples_of_num(n, m):\n",
      "    return [n * i for i in range(1, m + 1)]...\n",
      "\n",
      "Original: compiled=True, passed=False\n",
      "New: compiled=True, passed=False\n",
      "\n",
      "============================================================\n",
      "Problem #16: Write a python function to remove all digits from ...\n",
      "----------------------------------------\n",
      "Original:\n",
      "def remove(lst):\n",
      "    return [s for s in lst if not any(char.isdigit() for char in s)]...\n",
      "\n",
      "New:\n",
      "def remove(lst):\n",
      "    return [s for s in lst if not any(char.isdigit() for char in s)]...\n",
      "\n",
      "Original: compiled=True, passed=False\n",
      "New: compiled=True, passed=False\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T13:44:27.900191Z",
     "start_time": "2026-02-24T13:12:16.917997Z"
    }
   },
   "source": [
    "# Re-evaluate with new prompt\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RE-EVALUATING WITH NEW PROMPT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "new_compiled = 0\n",
    "new_passed = 0\n",
    "new_valid = 0\n",
    "\n",
    "for i in tqdm(range(N), desc=\"Re-evaluating\"):\n",
    "    prob = mbpp['test'][i]\n",
    "    ex_tests, ev_test, valid = split_tests(prob)\n",
    "    \n",
    "    if not valid:\n",
    "        continue\n",
    "    \n",
    "    new_valid += 1\n",
    "    code = generate_code_v2(prob['text'], ex_tests)\n",
    "    c, p = test_code(code, ev_test)\n",
    "    \n",
    "    new_compiled += int(c)\n",
    "    new_passed += int(p)\n",
    "\n",
    "new_compile_rate = new_compiled / new_valid if new_valid > 0 else 0\n",
    "new_pass_rate = new_passed / new_valid if new_valid > 0 else 0\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"COMPARISON\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Original: compile={compile_rate:.1%}, pass={pass_rate:.1%}\")\n",
    "print(f\"New: compile={new_compile_rate:.1%}, pass={new_pass_rate:.1%}\")\n",
    "\n",
    "diff = new_pass_rate - pass_rate\n",
    "if diff > 0:\n",
    "    print(f\"\\nImprovement: +{diff*100:.1f}%\")\n",
    "elif diff < 0:\n",
    "    print(f\"\\nDecline: {diff*100:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nNo change\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RE-EVALUATING WITH NEW PROMPT\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Re-evaluating:   6%|▌         | 6/100 [01:08<15:30,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a match!\n",
      "Not matched!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Re-evaluating:  98%|█████████▊| 98/100 [31:13<00:47, 23.68s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 20, 11, 24, 25, 24, 15, 4, 5, 26, 29, 54, 48, 56, 25, 110, 233, 154]\n",
      "[1, 1, 2, 3, 4, 5, 5, 6, 7, 7, 8, 8, 9, 11, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Re-evaluating: 100%|██████████| 100/100 [32:10<00:00, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPARISON\n",
      "==================================================\n",
      "Original: compile=100.0%, pass=72.0%\n",
      "New: compile=98.0%, pass=62.0%\n",
      "\n",
      "Decline: -10.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q2 Answer\n\nWe designed an alternative prompt (`generate_code_v2`) with a more compact, structured format — removing the lengthy instruction preamble and replacing it with a tightly formatted task/function/tests block. We then re-ran it on the same three failed cases and compared against the original generated code.\n\n**Results on the 3 failed cases (from output above):**\n\n| Problem | Original: compiled / passed | New: compiled / passed |\n|---------|----------------------------|------------------------|\n| #2 `count_common` | True / **False** | True / **False** |\n| #10 `multiples_of_num` | True / **False** | True / **False** |\n| #16 `remove` digits | True / **False** | True / **False** |\n\nNone of the three cases improved with the new prompt. For `count_common` and `multiples_of_num`, the new prompt produced code that is essentially the same logic under a different style — the model's misunderstanding of the problem was too fundamental for a surface-level prompt change to fix. For `remove`, both versions generated the same filter-based code that drops entire strings instead of stripping digits from inside them.\n\n**Re-evaluation on all 100 problems:**\n\n| | Compile rate | Pass rate |\n|--|--|--|\n| **Original prompt** | 100.0% | **72.0%** |\n| **New prompt (v2)** | 98.0% | **62.0%** |\n| **Difference** | −2.0% | **−10.0%** |\n\nThe new prompt led to a **decline of 10 percentage points** in pass rate and also caused 2 problems that previously compiled to fail compilation. This result is instructive — the original prompt's longer, more explicit structure (\"Function name must be:\", \"Example tests:\", \"Write only the function code:\") turns out to be better suited to this 1.5B model. The more concise v2 prompt apparently removes just enough guidance that the model occasionally generates explanatory text or incorrect syntax rather than clean code.\n\nThe main takeaway is that with small instruction-tuned models, **more explicit prompting beats concise prompting**. The model benefits from being told exactly what to do in each part of the prompt, rather than being expected to infer it from a compact format. The self-verification + retry strategy from Qu4 remains the most effective intervention, as it uses the test outputs themselves as a feedback signal rather than relying on the prompt wording alone."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T13:44:27.972317Z",
     "start_time": "2026-02-24T13:44:27.971121Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
