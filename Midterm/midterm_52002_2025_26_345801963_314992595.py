# -*- coding: utf-8 -*-
"""MidTerm_52002_2025_26_345801963_314992595.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nF6jt8D3WgzrpipYgZvky3rs_NVQ7fUU

# Midterm 52002 - 2025-2026

**General instructions:** The mid-term should be done in pairs. Submit your solutions in the course by uploading the solution files to the course Moodle page by 21.12.2025

**.ipynb solution file:** Fill-in the missing code blocks (for parts 1 and 3)and text blocks (for all three parts) in this ipynb notebook, change the name to `MidTerm_52002_2025_26_<ID1>_<ID2>.ipynb` where replace `<ID1>` and `<ID2>` by your ID nuymbers. Run the filled-notebook it in jupyter notebooks/google colab and upload to moodle the **filled** notebook with results (tables, graphs etc.).

**In addition**, submit a pdf/html export of the executed notebook with all the output, named `MidTerm_52002_2025_26_<ID1>_<ID2>.html` (or `.pdf`). Failing to submit both files as instructed will lead to a reduction in your midtem grade.


**Good luck!**

# **BigQuery & SQL - 40 points**

BigQuery is Google’s serverless data warehouse for large-scale analytics using standard SQL. In this part of the midterm you will query several public BigQuery tables from Python using the BigQuery client API (see the tutorial and example notebook from week 1). Use the **sandbox** environment and restrict yourself to **public datasets only**.

**Please note:**  
Your BigQuery resources are limited to `1 TB` of processed data per user per month—be careful with how many queries you run and how much data each query scans.

**Public New York City datasets used in this section:**

1. `bigquery-public-data.new_york_taxi_trips` – trip-level records for yellow, green, and for-hire vehicle (FHV) trips. We will mainly use the following trip tables  
   - `tlc_yellow_trips_*` (yellow),  
   - `tlc_green_trips_*` (green),  
   - `tlc_fhv_trips_*` (FHVs),  
   and the zone geometry table `taxi_zone_geom` (`zone_id`, `zone_name`, `borough`, `service_zone`) to map location IDs to boroughs and service zones.  

2. `bigquery-public-data.new_york_mv_collisions.nypd_mv_collisions` – NYPD motor vehicle collision reports with timestamps, location (latitude/longitude), borough, and other attributes used to study collisions by taxi zone.

These datasets overlap in both **space** (zones/boroughs via `taxi_zone_geom`) and **time** (years 2012, 2014, 2016), which allows you to analyze how different services operate and how they relate to collision patterns.

**Context (what are these services?):**
- **Yellow taxis** are the traditional cabs that historically served mainly Manhattan and airports.
- **Green taxis** ("boro cabs") were introduced later and are allowed to pick up passengers signaling them to stop at a street, primarily outside the Manhattan core. introduced in 2013.
- **FHVs (for-hire vehicles)** are application‑based or dispatch services (e.g., Uber/Lyft–like) that cannot pick up passengers from a street but can accept pre‑arranged trips. introduced in 2015.

**Guidelines:**
1. Fill this notebook with Python code cells (including SQL queries) in the designated places and run it in a Jupyter environment (e.g., Google Colab).  
2. Write efficient SQL queries and code; points may be deducted for unnecessarily expensive queries.  
3. Use the Python BigQuery API to send SQL to BigQuery and bring back results as pandas `DataFrame`s for further analysis and plotting. It is recommended to prototype queries first in the BigQuery web User Interface in your browser and then copy them into the notebook.  
4. All plots must have clear titles and labeled axes. You may use `matplotlib` or `pandas.DataFrame.plot`.  
5. When referencing BigQuery tables, wrap the fully-qualified table name in backticks (``). For example,  
   `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016`.  
   Always include appropriate date filters (e.g., on `pickup_datetime`) and, when referencing tables that span multiple years (e.g. ending in `*`), use the `_TABLE_SUFFIX` pseudo-column to filter specific tables (e.g., `AND _TABLE_SUFFIX = '2016'`) to keep scanned data within your sandbox quota.

**Instructions**
1. `from google.colab import auth` → `auth.authenticate_user()` (already provided below) and set `PROJECT_ID` to the sandbox project you created.
2. Instantiate the client with `bigquery.Client(project=PROJECT_ID)` and verify the same project is open in the BigQuery web UI.
3. Before running heavy queries, issue a dry-run (`client.query(..., job_config=bigquery.QueryJobConfig(dry_run=True, use_query_cache=False))`) or at least add `LIMIT` clauses to confirm the schema.
4. Date filters = smaller scans. Every tlc_yellow_trips_YYYY table is partitioned by pickup_datetime. If you add a clause limiting dates, for example: <br>
**WHERE pickup_datetime BETWEEN '2016-01-01' AND '2016-01-31'** <br>
then BigQuery touches only January’s partition instead of the whole 2016 table, so you don’t blow through the 1 TB sandbox limit. Use the same idea for dropoff_datetime or crash_date in the collisions table: always restrict the date range you query.
5. When merging the datasets, pre-aggregate it in SQL (by borough, month, ZIP) before bringing results into pandas to avoid downloading millions of rows.

**Necessary Libraries:**  
Below are a few libraries needed throught the MidTerm. Additional libraries are included later in other code blocks as needed. <br>
You may add additional libraries if needed, but have to explain fro each library what is its purpose and how is it used in your solution. It is your responsibility that the library performs the precise computation/analysis/visualization as requested in the questions.
"""

import pandas as pd
import matplotlib as plt
from google.cloud import bigquery
from sqlite3 import connect

"""### Q0: BigQuery client & Initital Query (2 points)"""

from google.colab import auth
auth.authenticate_user()
print('Authenticated')

"""Construct a BigQuery client object using the `client` method of the `bigquery` module.  The project name you use here should match the name of the project you open in the BigQuery environment. You should use in both places the same **non-huji** google user name"""

# complete
PROJECT_ID = "datamining-477016"
client = bigquery.Client(project=PROJECT_ID)

"""### Q1: Service/year availability check (6 points)
Use the three trip families below. All of them expose a `pickup_datetime` field you can filter by year.

- Yellow: `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_*`
- Green: `bigquery-public-data.new_york_taxi_trips.tlc_green_trips_*`
- For-hire vehicles (FHV): `bigquery-public-data.new_york_taxi_trips.tlc_fhv_trips_*`

Create a single BigQuery query that counts trips per `(year, service)` for the years **2012, 2014, 2016**. The result should be a pandas DataFrame named `service_year_counts` containing the columns `year`, `service` (with values `'yellow'`, `'green'`, `'fhv'`), and `num_trips`. The table should be in "long format", meaning each row corresponds to a unique combination of year and service (e.g., one row for 2012-yellow, another for 2012-green, etc.), rather than having separate columns for each service. Make sure missing combinations (e.g., FHV in 2012) still appear with `0` trips.
"""

# complete

service_year_counts_query = """SELECT
  EXTRACT(YEAR FROM pickup_datetime) AS year,
  'yellow' AS service,
  COUNT(*) AS num_trips
FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_*`
WHERE _TABLE_SUFFIX IN ('2012','2014','2016')
GROUP BY year

UNION ALL

SELECT
  EXTRACT(YEAR FROM pickup_datetime) AS year,
  'green' AS service,
  COUNT(*) AS num_trips
FROM `bigquery-public-data.new_york_taxi_trips.tlc_green_trips_*`
WHERE _TABLE_SUFFIX IN ('2012','2014','2016')
GROUP BY year

UNION ALL

SELECT
  EXTRACT(YEAR FROM pickup_datetime) AS year,
  'fhv' AS service,
  COUNT(*) AS num_trips
FROM `bigquery-public-data.new_york_taxi_trips.tlc_fhv_trips_*`
WHERE _TABLE_SUFFIX IN ('2012','2014','2016')
GROUP BY year;"""


query_job = client.query(service_year_counts_query)
service_year_counts = query_job.result().to_dataframe()

import pandas as pd

years = [2012, 2014, 2016]
services = ['yellow', 'green', 'fhv']

# toutes les combinaisons possibles (3×3 = 9 lignes)
full_index = pd.MultiIndex.from_product(
    [years, services],
    names=['year', 'service']
)

# réindexer et mettre 0 lorsque la combinaison n'existe pas
service_year_counts = (
    service_year_counts
    .set_index(['year', 'service'])
    .reindex(full_index, fill_value=0)
    .reset_index()
)

service_year_counts

"""### Q2: Trip volumes by service 2012,2014,2016 (8 points)
Using the `service_year_counts` table/DataFrame you created in Q1:

1. For each year, compute the total number of trips across all services.
2. For each `(year, service)` pair, compute the share of trips contributed by that service out of all trips in all years and services.
3. Present the results as a tidy table with the columns `year`, `service`, `total_trips`, `share`.
4. In your verbal answer, comment on:
   - how total trip volumes evolve across the three years,
   - how the composition shifts when green service (2014) and FHVs (2016) enter the market.

**Answer:**
"""

# complete
# Assuming `service_year_counts` (year, service, num_trips) was created in Q1
df = service_year_counts.copy()
df["total_trips"] = df.groupby("year")["num_trips"].transform("sum")
grand_total = df["num_trips"].sum()
df["share"] = df["num_trips"] / grand_total
df.sort_values(["year", "service"], inplace=True)
df

# q2 = # YOUR ANSWER HERE

#q2



"""### Interpret the table: report the trip totals for each service in 2012, 2014, and 2016, and describe how each service’s market share changes across the three years:
YOR ANSWER HERE


1. Evolution of total trip volumes across the three years

Total trip volumes grow substantially from 2012 to 2016.
In 2012, only yellow taxis operate, so overall trip volume equals yellow’s volume.
By 2014, the total number of trips increases, driven by a growing market and the introduction of the green taxi service, which adds extra trips on top of the existing yellow volume.
By 2016, total trip volume reaches its highest level: yellow and green continue to operate, and the entry of FHV services (for-hire vehicles) further expands the market.
Overall, the data suggest a steady and significant increase in total trip activity over the three selected years.

⸻

2. How the market composition shifts when green (2014) and FHVs (2016) enter

In 2012, the market is entirely dominated by yellow taxis (100% of trips).

In 2014, the arrival of the green service changes the composition:
yellow still accounts for the majority of trips, but green taxis take a noticeable share, reducing yellow’s dominance.
The distribution becomes more diversified, with the market no longer being a single-service monopoly.

In 2016, the introduction of FHVs causes the most dramatic shift.
FHVs immediately capture a large share of total trips, while the relative shares of both yellow and green decline.
The market becomes more competitive and multi-service, with FHVs emerging as a major segment alongside traditional taxis.

### Q3: Which service dominates each **zone**? (8 points)

In this question you will work at the **taxi-zone level** (not borough level) and compare which service has the most pickups in each zone and year.

1. Using the original trip tables and the zone geometry table  
   `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom`:
   - For **yellow** and **green** taxis, map trips to zones by joining  
     `pickup_location_id` in the trip tables with `zone_id` in `taxi_zone_geom`.
   - For **FHV** trips, map trips to the same zones by joining the appropriate location field  
     (e.g. `location_id`) with `zone_id` in `taxi_zone_geom`.

2. For each of the years **2012**, **2014**, and **2016**:
   - Count the number of pickups per combination of   
     `(zone_id, zone_name, borough, service)`  
     (do **not** aggregate only by borough; keep the **zone** as the basic spatial unit).
   - For each zone and year, determine which service has the **largest** number of pickups  
     (i.e., the “dominating” service in that zone-year).

3. Create a summary table (or DataFrame) that, for each pair of year and zone, contains at least:  
   - `year`  
   - `zone_id`  
   - `zone_name`  
   - `borough`  
   - `dominating_service`  
   - the corresponding pickup count for that service in that zone-year.

4. Briefly describe the results:  
   - How does the pattern of which service dominates each **zone** change between 2012, 2014, and 2016?  
   - Which parts of the city appear to shift from one dominating service to another over time?
"""

# complete
zone_dominance_2012_query = """
-- YELLOW
SELECT
  2012 AS year,
  z.zone_id,
  z.zone_name,
  z.borough,
  'yellow' AS service,
  COUNT(*) AS pickups
FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2012` AS t
JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` AS z
  ON t.pickup_location_id = z.zone_id
GROUP BY z.zone_id, z.zone_name, z.borough
"""


zone_dominance_2012 = client.query(zone_dominance_2012_query).result().to_dataframe()
zone_dominance_2012.head()

# complete
zone_dominance_2014_query = """

-- YELLOW
SELECT
  2014 AS year,
  z.zone_id,
  z.zone_name,
  z.borough,
  'yellow' AS service,
  COUNT(*) AS pickups
FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2014` AS t
JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` AS z
  ON t.pickup_location_id = z.zone_id
GROUP BY z.zone_id, z.zone_name, z.borough

UNION ALL

-- GREEN
SELECT
  2014 AS year,
  z.zone_id,
  z.zone_name,
  z.borough,
  'green' AS service,
  COUNT(*) AS pickups
FROM `bigquery-public-data.new_york_taxi_trips.tlc_green_trips_2014` AS t
JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` AS z
  ON t.pickup_location_id = z.zone_id
GROUP BY z.zone_id, z.zone_name, z.borough;
"""

zone_dominance_2014 = client.query(zone_dominance_2014_query).result().to_dataframe()
zone_dominance_2014.head()

# complete
# Q3 – 2016

zone_dominance_2016_query = """

-- YELLOW 2016
SELECT
  2016 AS year,
  z.zone_id,
  z.zone_name,
  z.borough,
  'yellow' AS service,
  COUNT(*) AS pickups
FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016` AS t
JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` AS z
  ON CAST(t.pickup_location_id AS STRING) = CAST(z.zone_id AS STRING)
GROUP BY z.zone_id, z.zone_name, z.borough

UNION ALL

-- GREEN 2016
SELECT
  2016 AS year,
  z.zone_id,
  z.zone_name,
  z.borough,
  'green' AS service,
  COUNT(*) AS pickups
FROM `bigquery-public-data.new_york_taxi_trips.tlc_green_trips_2016` AS t
JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` AS z
  ON CAST(t.pickup_location_id AS STRING) = CAST(z.zone_id AS STRING)
GROUP BY z.zone_id, z.zone_name, z.borough

UNION ALL

-- FHV 2016
SELECT
  2016 AS year,
  z.zone_id,
  z.zone_name,
  z.borough,
  'fhv' AS service,
  COUNT(*) AS pickups
FROM `bigquery-public-data.new_york_taxi_trips.tlc_fhv_trips_2016` AS t
JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` AS z
  ON CAST(t.location_id AS STRING) = CAST(z.zone_id AS STRING)
GROUP BY z.zone_id, z.zone_name, z.borough
"""


zone_dominance_2016 = client.query(zone_dominance_2016_query).result().to_dataframe()
zone_dominance_2016.head()

"""### What patterns do you see in the results?
YOUR ANSWER HERE

Across all three years (2012, 2014, and 2016), the same general pattern appears:
	•	The zones with the highest number of pickups are consistently located in Manhattan.
	•	The dominance of Manhattan strengthens over time, with Midtown and nearby areas becoming increasingly prominent by 2016.
	•	Zones from Queens, the Bronx, or Staten Island occasionally appear in earlier years but their pickup volumes are always much lower.
	•	In all three datasets, the dominating service is always Yellow Taxi, because the FHV 2016 dataset does not contain zone identifiers and therefore does not contribute to zone-level dominance.
"""

summary = pd.concat([
    zone_dominance_2012,
    zone_dominance_2014,
    zone_dominance_2016
], ignore_index=True)

summary = summary[[
    "year", "zone_id", "zone_name", "borough", "service", "pickups"
]]

# 2) Pour chaque zone-year, sélectionner le service dominant
dominating = (
    summary.sort_values("pickups", ascending=False)
           .groupby(["year", "zone_id"], as_index=False)
           .first()
)

dominating = dominating.rename(columns={
    "service": "dominating_service",
    "pickups": "dominating_pickups"
})

# print(dominating.head())
print(dominating.groupby(["year", "dominating_service"]).size())

"""- How does the pattern of which service dominates each **zone** change between 2012, 2014, and 2016?  

  	Based on the summary table, the distribution of dominating services changes substantially across the three years. In 2012, every zone is dominated by yellow taxis (260 zones), meaning yellow is the universal leading service across the city. By 2014, this pattern shifts: although yellow taxis still dominate most zones (156 zones), a large number of zones (104 zones) are now dominated by green taxis. This reflects the expansion of the green taxi program outside Manhattan. By 2016, the landscape changes again, with for-hire vehicles (FHV) becoming the dominant service in the majority of zones (200 zones). Yellow taxis dominate far fewer zones (54), and green taxis remain dominant in only a small number of zones (6). Overall, the pattern shows a clear transition from full yellow-taxi dominance in 2012, to a mixed yellow–green landscape in 2014, and finally to widespread FHV dominance in 2016.

   - Which parts of the city appear to shift from one dominating service to another over time?

   	The year-to-year changes suggest strong geographical transitions in service dominance. Between 2012 and 2014, many zones outside Manhattan—particularly in Queens, Brooklyn, the Bronx, and Upper Manhattan—shift from yellow-taxi dominance to green-taxi dominance, consistent with the creation of the green-taxi program aimed at the outer boroughs. Between 2014 and 2016, most of these same zones shift again, this time from green taxis to FHVs, reflecting the rapid rise of Uber and Lyft. Even in Manhattan, several zones that were yellow-dominated in both 2012 and 2014 become FHV-dominated by 2016. Only the most central and tourist-heavy areas remain yellow-dominated. These shifts illustrate a clear evolution: yellow → green → FHV across much of the city.

### Q4: Did new services actually *displace* the old ones? (8 points)

In Q3 you computed which service dominates each zone for 2012, 2014, and 2016. Here you will reuse those dominance labels, and additionally query the yellow trip counts per zone per year (for zones where yellow may not be dominant), to ask whether new services reduce the use of existing ones using a difference-in-differences comparison at the zone level.


#### Notation

Let $TY_{z,t}$ denote the number of **yellow taxi trips** in zone $z$ in year $t$. Define the **growth rate** of yellow trips in zone $z$ between years $t_0$ and $t_1$ as:

$$g_{z}^{(t_0 \to t_1)} = \frac{TY_{z,t_1} - TY_{z,t_0}}{TY_{z,t_0}}$$

#### 4a. Diff-in-diff for yellow → green → FHV

Using your **zone–year–service table** and your **dominating-service labels**:

1. Construct a panel DataFrame - a table where each row represents a unique (zone_id, year) pair, allowing us to track how the same zones change over time - where each row represents a unique `(zone_id, year)` pair, containing columns for yellow, green, and FHV trip counts. For years where a service did not exist (e.g., FHV in 2012), set the count to `0`.

2. **Stage 1 (2012 → 2014) — Green entry:**

   Define the groups based on dominance:
   - $G_{\text{treat}}$: zones that are yellow-dominated in 2012 and green-dominated in 2014  
   - $G_{\text{ctrl}}$: zones that are yellow-dominated in both 2012 and 2014

   Compute the DiD as the difference in **average growth rates** between groups:

   $$\Delta\Delta_{\text{yellow}}^{(2012 \to 2014, G)} = \frac{1}{|G_{\text{treat}}|}\sum_{z \in G_{\text{treat}}} g_{z}^{(2012 \to 2014)} - \frac{1}{|G_{\text{ctrl}}|}\sum_{z \in G_{\text{ctrl}}} g_{z}^{(2012 \to 2014)}$$

3. **Stage 2 (2014 → 2016) — FHV entry:**

   Define the groups:
   - $F_{\text{treat}}$: zones that are yellow-dominated in 2014 and FHV-dominated in 2016  
   - $F_{\text{ctrl}}$: zones that are yellow-dominated in 2014 and remain yellow-dominated in 2016

   Compute:

   $$\Delta\Delta_{\text{yellow}}^{(2014 \to 2016, F)} = \frac{1}{|F_{\text{treat}}|}\sum_{z \in F_{\text{treat}}} g_{z}^{(2014 \to 2016)} - \frac{1}{|F_{\text{ctrl}}|}\sum_{z \in F_{\text{ctrl}}} g_{z}^{(2014 \to 2016)}$$

4. Report both DiD estimates and briefly comment on whether they suggest that green displaced yellow, and that FHV displaced taxis.

---

**Clarification:** In both stages, the *outcome* is the **rate of change in yellow trip counts**. "Green" (Stage 1) and "FHV" (Stage 2) enter only through the **treatment definition** (zones that switch dominance), not as separate outcome variables.

---

#### 4b. Interpretation and assumptions

Based on your results in 4a and the dominance maps from Q3: Summarize, in words, what your DiD estimates say about how green taxis affected yellow usage, and how FHV affected taxi usage.


---

(An interesting question to ask yourself: what additional assumptions do we need to claim that the new service caused the change in yellow trips, rather than just being correlated with it?)
"""

# Complete

import statsmodels.formula.api as smf


zone_service_trips_sql = """

WITH base AS (

  -- Yellow trips
  SELECT
    z.zone_id,
    EXTRACT(YEAR FROM t.pickup_datetime) AS year,
    COUNT(*) AS yellow_trips,
    0 AS green_trips,
    0 AS fhv_trips
  FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_*` t
  JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` z
    ON CAST(t.pickup_location_id AS STRING) = CAST(z.zone_id AS STRING)
  WHERE _TABLE_SUFFIX IN ('2012','2014','2016')
  GROUP BY zone_id, year

  UNION ALL

  -- Green trips
  SELECT
    z.zone_id,
    EXTRACT(YEAR FROM t.pickup_datetime) AS year,
    0 AS yellow_trips,
    COUNT(*) AS green_trips,
    0 AS fhv_trips
  FROM `bigquery-public-data.new_york_taxi_trips.tlc_green_trips_*` t
  JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` z
    ON CAST(t.pickup_location_id AS STRING) = CAST(z.zone_id AS STRING)
  WHERE _TABLE_SUFFIX IN ('2012','2014','2016')
  GROUP BY zone_id, year

  UNION ALL

  -- FHV trips
  SELECT
    z.zone_id,
    EXTRACT(YEAR FROM t.pickup_datetime) AS year,
    0 AS yellow_trips,
    0 AS green_trips,
    COUNT(*) AS fhv_trips
  FROM `bigquery-public-data.new_york_taxi_trips.tlc_fhv_trips_*` t
  JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` z
    ON CAST(t.location_id AS STRING) = CAST(z.zone_id AS STRING)
  WHERE _TABLE_SUFFIX IN ('2012','2014','2016')
  GROUP BY zone_id, year
)

SELECT
  zone_id,
  year,
  SUM(yellow_trips) AS yellow,
  SUM(green_trips) AS green,
  SUM(fhv_trips) AS fhv
FROM base
GROUP BY zone_id, year
ORDER BY zone_id, year;

"""
zone_service_trips = client.query(zone_service_trips_sql).result().to_dataframe()
# ADD MERGE WITH RELEVANT DATA FROM Q3
panel = zone_service_trips.sort_values(['zone_id','year']).reset_index(drop=True)
dom_2012 = zone_dominance_2012[['zone_id','service']].rename(columns={'service':'dom_2012'})
dom_2014 = zone_dominance_2014[['zone_id','service']].rename(columns={'service':'dom_2014'})
dom_2016 = zone_dominance_2016[['zone_id','service']].rename(columns={'service':'dom_2016'})

dominance = (
    dom_2012
    .merge(dom_2014, on='zone_id', how='outer')
    .merge(dom_2016, on='zone_id', how='outer')
)
dominance
# CREATE HELPER FUNCTION FOR DiD

# g(t0→t1) = (TY_t1 - TY_t0) / TY_t0
def growth_rate(panel, t0, t1):
    t0_series = panel[panel.year == t0].set_index('zone_id')['yellow']
    t1_series = panel[panel.year == t1].set_index('zone_id')['yellow']
    g = (t1_series - t0_series) / t0_series
    return g.dropna()

# HELPER FUNCTION: DID
def did(growth_series, treated_ids, control_ids):
    treated_mean = growth_series.loc[treated_ids].mean()
    control_mean = growth_series.loc[control_ids].mean()
    return treated_mean - control_mean




# Stage 1: 2012→2014 - yellow to grean
treated_2012_2014 = dominance[
    (dominance.dom_2012 == 'yellow') &
    (dominance.dom_2014 == 'green')
]['zone_id'].unique()

control_2012_2014 = dominance[
    (dominance.dom_2012 == 'yellow') &
    (dominance.dom_2014 == 'yellow')
]['zone_id'].unique()

# Compute growth rates
g_2012_2014 = growth_rate(panel, 2012, 2014)

# Compute DiD
did_2012_2014 = did(g_2012_2014, treated_2012_2014, control_2012_2014)
print("DiD (2012→2014, Green entry):", did_2012_2014)

# Stage 2 : 2014→2016 - yellow to blue
treated_2014_2016 = dominance[
    (dominance.dom_2014 == 'yellow') &
    (dominance.dom_2016 == 'fhv')
]['zone_id'].unique()

control_2014_2016 = dominance[
    (dominance.dom_2014 == 'yellow') &
    (dominance.dom_2016 == 'yellow')
]['zone_id'].unique()

# Compute growth rates
g_2014_2016 = growth_rate(panel, 2014, 2016)

# Compute DiD
did_2014_2016 = did(g_2014_2016, treated_2014_2016, control_2014_2016)
print("DiD (2014→2016, FHV entry):", did_2014_2016)

"""### Q4b
YOUR ANSWER HERE

The DiD estimate for 2012–2014 is about 0.0048, meaning that zones where Green taxis became dominant experienced a growth in Yellow taxi trips only 0.48% higher than zones that remained Yellow-dominated. This difference is extremely small. It suggests that the introduction of Green taxis did not meaningfully reduce Yellow taxi usage and may have had almost no competitive displacement effect. In practice, Yellow usage evolved very similarly in treated and control zones—indicating either that Yellow taxis kept their customer base or that Green taxis mainly served areas or riders that Yellow taxis were not serving anyway. To interpret this as a causal conclusion, we would need the parallel-trends assumption: that, without Green taxis, both types of zones would have followed the same trend in Yellow usage.

Between 2014 and 2016, the DiD estimate is roughly 0.0017, again close to zero. This implies that zones where FHV services became dominant experienced Yellow trip growth only 0.17% higher than zones that remained Yellow-dominated—another extremely weak effect. Despite the explosive rise of FHV services citywide during this period, the zone-level data suggest that their entry did not significantly decrease Yellow taxi usage in the short run. Yellow demand appears surprisingly stable, even in zones where FHV became dominant. As before, interpreting this as a causal effect requires strong assumptions: notably, that no other zone-specific factors affected Yellow usage differently across treated and control zones, and that dominance shifts reflect genuine market substitution rather than other unrelated changes.

### Q5: Collisions by zone and the rise of for-hire vehicles (8 points)

**5a. Trends in collisions by zone**

Using `bigquery-public-data.new_york_mv_collisions.nypd_mv_collisions` and `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom`:

**Task:** Write a single BigQuery SQL query that:

1. **Maps each collision to a taxi zone** using a spatial join: use `ST_CONTAINS(tz.zone_geom, ST_GEOGPOINT(c.longitude, c.latitude))` to match collisions to zones.
2. **Aggregates by year and zone**: For years **2012**, **2014**, and **2016**, count collisions per `(zone_id, zone_name, borough)`. Filter by year using `EXTRACT(YEAR FROM crash_date)`.

**Expected Output:** A DataFrame `collisions_zone_year` with columns: `year`, `zone_id`, `zone_name`, `borough`, `num_collisions` (one row per zone-year).

3. **Visualization:** Run the provided code below to create three side-by-side maps (2012, 2014, 2016) showing collision counts per zone. The code merges your DataFrame with zone geometries from Q3.
---

 **Q5b: Do collisions align more with some services than others?**

Using your 2016 zone-level panel (with `num_collisions` and trip counts per service):

1. For each service (`yellow`, `green`, `fhv`), create a log–log scatter plot of  
   **x-axis:** number of trips in 2016 (per zone)  
   **y-axis:** number of collisions in 2016 (per zone).  

2. Place the three scatter plots side by side so you can visually compare patterns across services.

3. Briefly answer:
   - For which service does collision count appear most strongly associated with trip volume?
   

---

**Q5c: Alternative explanations**  
What can be an alternative interpretation of the association you find in **5b**, without treating trips of a certain service themselves as the direct cause of more collisions?  

Give at least one plausible story or mechanism that could generate the pattern you see.

*Hint:* Look back at your collision maps from **5a** and your service-dominance maps from the earlier question 3.
"""

# Complete

collisions_zone_year_sql = """
SELECT
  EXTRACT(YEAR FROM c.timestamp) AS year,
  tz.zone_id,
  tz.zone_name,
  tz.borough,
  COUNT(*) AS num_collisions
FROM `bigquery-public-data.new_york_mv_collisions.nypd_mv_collisions` AS c
JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` AS tz
  ON ST_CONTAINS(tz.zone_geom, ST_GEOGPOINT(c.longitude, c.latitude))
WHERE EXTRACT(YEAR FROM c.timestamp) IN (2012, 2014, 2016)
GROUP BY year, zone_id, zone_name, borough
ORDER BY year, zone_id;
"""

collisions_zone_year = client.query(collisions_zone_year_sql).result().to_dataframe()
collisions_zone_year.head()

zones_query = """
SELECT
  zone_id,
  zone_name,
  borough,
  ST_AsText(zone_geom) AS geom_wkt
FROM `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom`;
"""

zones_df = client.query(zones_query).to_dataframe()
zones_df["geometry"] = zones_df["geom_wkt"].apply(wkt.loads)

zones_gdf = gpd.GeoDataFrame(
    zones_df[["zone_id", "zone_name", "borough", "geometry"]],
    geometry="geometry",
    crs="EPSG:4326"
)

zones_gdf.head()

# Visualization Code (Run this after creating your dataframe)
# Ensure your dataframe is named 'collisions_zone_year'
# Ensure 'zones_gdf' is available from previous steps

import geopandas as gpd
import matplotlib.pyplot as plt

# 1) Make sure zone_id types match and attach geometry
if 'zones_gdf' in locals():
    zones_for_merge = zones_gdf[["zone_id", "geometry"]].copy()
    zones_for_merge["zone_id"] = zones_for_merge["zone_id"].astype(str)

    collisions_geo = collisions_zone_year.copy()
    collisions_geo["zone_id"] = collisions_geo["zone_id"].astype(str)

    collisions_geo = collisions_geo.merge(
        zones_for_merge,
        on="zone_id",
        how="left"
    )

    collisions_geo = gpd.GeoDataFrame(
        collisions_geo,
        geometry="geometry",
        crs="EPSG:4326"
    )

    # 2) Plot three maps side by side
    years = [2012, 2014, 2016]
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))

    # Use common max so "darkness" is comparable across years
    vmax = collisions_geo["num_collisions"].max()

    for ax, yr in zip(axes, years):
        gsub = collisions_geo[collisions_geo["year"] == yr].dropna(subset=["geometry"])
        if not gsub.empty:
            gsub.plot(
                column="num_collisions",
                ax=ax,
                cmap="Greys",        # darker = more collisions
                vmax=vmax,
                legend=(yr == 2016), # show legend only on last panel
                edgecolor="black",
                linewidth=0.1
            )
        ax.set_title(f"Collisions by zone – {yr}")
        ax.axis("off")

    plt.tight_layout()
    plt.show()
else:
    print("zones_gdf not found. Please ensure you loaded the zone geometries in Question 3.")

"""### 5c
YOUR ANSWER HERE

A plausible alternative explanation is that the relationship between dominant services and higher collision counts is driven by underlying demand and traffic density, rather than by the services themselves causing more collisions. The zones where a service (yellow in 2012, green in 2014, and especially FHV in 2016) dominates are also the zones with the highest levels of mobility, commercial activity, nightlife, and overall congestion. These zones naturally attract more vehicles of all types and more pedestrians, which mechanically increases the likelihood of collisions. In this interpretation, pickups and collisions are both consequences of high-demand, high-density environments. Therefore, the observed association does not necessarily imply that having more trips from a particular service causes more collisions; instead, both patterns may simply reflect the fact that busy areas generate both more rides and more accidents, independent of the specific taxi/FHV service operating there.

# **Unix  - 30 points**
Each question in this part requests **Unix shell commands** as an answer. Provide the exact commands you used, the **command output** (or the first few lines if long) and a short explanation for each question, in three well separated pargaraphs/blocks.

## Q1: Simple Counting (6 points)
 Copy the `review-Oregon.json.gz` file from the moriah cluster, located at
 the path `sci/courses/2025/52002/course_materials/midterm`.
 You may work at the moriah cluster (recommended) or any other environment such as your personal computer.
Extract the file's content, and use the `wc` command to display the number of lines, words, and characters in the `review-Oregon.json` file. Print the first line of the file and briefly explain what the data represents.

**YOUR SOLUTIONS HERE**

**Add Question 1 Shell Commands here:**
```
zcat review-Oregon.json.gz | wc -l
zcat review-Oregon.json.gz | wc -w
zcat review-Oregon.json.gz | wc -m
zcat review-Oregon.json.gz| head -1
```

**Add Question 1 Shell Output here:**
```
11012170 lines
424536800 words
3590454764 chars
{"user_id": "108990823942597776781", "name": "Richard Carroll", "time": 1604639688837, "rating": 5, "text": "Fabulous food and amazing service.  Will be back for more!  The beef ribs are killer.", "pics": null, "resp": null, "gmap_id": "0x80dce9997c8d25fd:0xc6c81c1983060cbc"}
```


**Explanation:**


The dataset contains more than 11 million customer reviews from Amazon users in Oregon. Each line is a JSON record representing one review, including fields such as review text, rating, product ID, user ID, timestamp, and other metadata. It is essentially a large-scale corpus of real-world product reviews used for NLP, text analysis, and recommendation research.

## Q2: Split the reviews by rating (6 points)
Split the `review-Oregon.json` file into 5 separate files based on their rating: `rating_1.json`, `rating_2.json`, ..., `rating_5.json`.
Each output file should contain **valid JSON objects** with only the `user_id` and `text` fields (you can drop the other fields to save space).

**Task:** Write a shell loop to create these 5 files from the main dataset. Print the first 2 lines of each splitted file

**Hint:** You can loop from 1 to 5, use `grep` to filter by rating, and then pipe to `jq` (or `sed`/`awk`) to construct the new smaller JSON objects.

**YOUR SOLUTIONS HERE**

**Add Question 2 Shell Commands here:**
```
for r in {1..5}; do
    echo "Processing rating $r"
    zcat review-Oregon.json.gz \
        | grep "\"rating\": $r" \
        | jq -c '{user_id:.user_id, text:.text}' \
        > rating_${r}.json

    echo "First 2 lines of rating_${r}.json:"
    head -n 2 rating_${r}.json
    echo "-----------------------------------"
    done
```

**Add Question 2 Shell Output here:**
```
Processing rating 1
First 2 lines of rating_1.json:
{"user_id":"105137373872660304656","text":"0 stars. I visited Dr McDonald for a consult in an extreme state of stress and anxiety, and he did nothing but intensify it and make me feel worse. He used fear tactics to scare me into paying for Invisalign treatment without having insurance to cover it. He made it sound like there was no other option and that it was an emergency or my teeth would fall out. so I gave him 4500 dollars out of pocket. I went into a financial crisis and developed an anxiety disorder. I later gained insurance, and asked their office for help filing a pre-auth to help cover it. I spoke with Shawna there, who was incredibly rude and condescending and basically laughed at me. She refused to file a preauth and refused to help me. I later complained, and the office completely ignored my complaint. They did not regard it or apologize. probably the worst medical experience of my life. If you’re an adult needing orthodontia treatment, and looking for someone who is compassionate with good bedside manor,  I recommend *anyone* but dr McDonald and his staff. So incredibly rude and distant. They are terrible and should be ashamed of themselves. They don’t care about you, just your money."}
{"user_id":"111315721700794904534","text":null}
-----------------------------------
Processing rating 2
First 2 lines of rating_2.json:
{"user_id":"110813163323535574099","text":"15 minute job cost $165.  They charge one hour minimum at $115 per hour plus $50 charge to plug in the computer.  Mechanic worked fast and did fix the problem.  The guys in the office took longer to create invoice and run the credit card then it took to fix my truck.  I don't feel like i was billed appropriately for the job that was done so i wanted to share my experience."}
{"user_id":"106754283817289328985","text":"Not impressed sat here all day for a job that should've taken an hour...."}
-----------------------------------
Processing rating 3
First 2 lines of rating_3.json:
{"user_id":"104222764848114766117","text":"Great food but I definitely would not suggest working here. They close more than they are open. The owner never gives you a straight answer and says IDK. That's so unprofessional. No warnings when they close either. Pretty shadey if you ask me."}
{"user_id":"106894759305652649705","text":"We were there at opening only ones good food the first young man we talked to was very personable and provided great customer service the young lady however that we ordered from and took our money was not personable she needs to be taught what customer service is my wife had to walk inside fro the deck area to get basic service herself once she brought our food we did not see her until we were ready to leave"}
-----------------------------------
Processing rating 4
First 2 lines of rating_4.json:
{"user_id":"103203861558418861267","text":"Abby loves coming to see the staff!"}
{"user_id":"116578519255526154646","text":"Efficient, short visits and easy scheduling"}
-----------------------------------
Processing rating 5
First 2 lines of rating_5.json:
{"user_id":"108990823942597776781","text":"Fabulous food and amazing service.  Will be back for more!  The beef ribs are killer."}
{"user_id":"113177950518394018158","text":"Great baked goods.  I recommend the cinnamon buns."}
```


**Explanation:**


The shell script loops through all rating values from 1 to 5 and creates one output file per rating. For each rating, the script decompresses the main dataset using zcat, filters the JSON-lines data to keep only the reviews that contain the target rating (using grep), and then transforms each review into a smaller JSON object that contains only the user_id and text fields (using jq). The resulting cleaned reviews are written into files named rating_1.json, rating_2.json, …, rating_5.json.

At the end of each iteration, the script prints the first two lines of the newly created file so the user can quickly verify that the filtering and formatting worked correctly. Overall, this loop automatically splits the large dataset into five compact files—one for each rating level—while keeping only the essential fields needed for the analysis.

## Q3: Compute the average review length (in words) (6 points)
For each of the 5 rating files you created in Q2, compute the **average number of words per review**.

**Task:** Write a command (or loop) that calculates this average for each file and prints it.

**Hint:** You can use `jq` to extract the `.text` field and `awk` (or `wc`) to count words and lines.

Does it seem that the review length is associated with the rating? explain

**YOUR SOLUTIONS HERE**

**Add Question 3 Shell Commands here:**
```
for r in {1..5}; do
    echo "Rating $r:"

    # Extract the text field, count total words and total reviews
    total_words=$(jq -r '.text' rating_$r.json | wc -w)
    total_reviews=$(jq -r '.text' rating_$r.json | wc -l)

    # Compute average
    avg=$(echo "scale=2; $total_words / $total_reviews" | bc)

    echo "Average review length: $avg words"
    echo "-----------------------------------"
done
```

**Add Question 3 Shell Output here:**
```
Rating 1:
Average review length: 37.57 words
-----------------------------------
Rating 2:
Average review length: 26.66 words
-----------------------------------
Rating 3:
Average review length: 13.50 words
-----------------------------------
Rating 4:
Average review length: 10.51 words
-----------------------------------
Rating 5:
Average review length: 15.05 words
-----------------------------------
```

**Explanation:**

The loop iterates over the five rating files created in Q2. For each file, it extracts all review texts using jq, counts the total number of words with wc -w, and counts the number of reviews with wc -l. Dividing these two values gives the average review length (in words) for that rating. The script prints the average for each rating from 1 to 5.

## Q4: Sentiment Scoring with a Lexicon (6 points)

We can approximate sentiment with a simple lexicon-based approach. You are given two word lists:
* **Positive words:** `positive_words.txt` (contains: `great, good, excellent, amazing, love, best, perfect, nice`)
* **Negative words:** `negative_words.txt` (contains: `bad, terrible, awful, poor, worst, hate, disappointing, waste`)

Your task is to calculate a sentiment score for each review based on these words. The score is defined as:

**Score = (count of positive words) - (count of negative words)**

Write a Python script that computes the **average sentiment score** for a given rating file. Your script should handle case-insensitivity (i.e., "Great" and "great" should both count). **You must run this on the cluster via SLURM, processing each rating file (`rating_1.json` through `rating_5.json`) as a separate job that can be run in parallel.**

In your solution, include:
1. A Python script `sentiment.py` that takes a rating file as input and prints its average sentiment score.
2. A SLURM job script `run_q4.sh` that runs your sentiment script on all 5 rating files **in parallel** (one task per file).
3. The exact submission command you used: `sbatch run_q4.sh`.
4. The top ~10 lines of the SLURM log output file (e.g., `slurm-<jobid>.out`) showing the printed averages.
5. Explanation and analysis of the results

Copy the content of all five stages to the designated blocks below

### Run this first in the Moriah command line to save the dictionaries into files
```bash
# Create lexicon files:
moriah-gw-01% cat > positive_words.txt << 'EOF'
great
good
excellent
amazing
love
best
perfect
nice
EOF

moriah-gw-01% cat > negative_words.txt << 'EOF'
bad
terrible
awful
poor
worst
hate
disappointing
waste
EOF
```
"""

### 1. Add Question 4 Python Script here (`sentiment.py`):

# save in your folder as sentiment.py
# Your Python code here that:
# 1. Takes a rating file path as a command-line argument
# 2. Loads the positive and negative word lists
# 3. Calculates the sentiment score for each review
# 4. Prints the average sentiment score for that file


import sys
import os
import json

POSITIVE_PATH = "positive_words.txt"
NEGATIVE_PATH = "negative_words.txt"

# Create lexicon files if they don't exist
if not os.path.exists(POSITIVE_PATH):
    with open(POSITIVE_PATH, 'w') as f:
        f.write('great\ngood\nexcellent\namazing\nlove\nbest\nperfect\nnice\n')

if not os.path.exists(NEGATIVE_PATH):
    with open(NEGATIVE_PATH, 'w') as f:
        f.write('bad\nterrible\nawful\npoor\nworst\nhate\ndisappointing\nwaste\n')

# Load lexicons, lowercase, strip newlines
with open(POSITIVE_PATH) as f:
    pos = set(word.strip().lower() for word in f)
with open(NEGATIVE_PATH) as f:
    neg = set(word.strip().lower() for word in f)

def calculate_sentiment(text):
    if not text:
      return 0
    score = 0
    # Split text into words, handling punctuation implicitly by stripping non-alphabetic chars
    words = [word.strip('.,;!?"').lower() for word in text.split()]
    for word in words:
        if word in pos:
            score += 1
        if word in neg:
            score -= 1
    return score

def calc_score(file_path):
    assert os.path.isfile(file_path), "Your path does not exist!"

    total = 0
    count = 0

    # NDJSON → one JSON object per line
    with open(file_path) as f:
        for line in f:
            rating = json.loads(line)
            # Ensure 'text' field exists and is not null before processing
            if 'text' in rating and rating['text'] is not None:
                total += calculate_sentiment(rating["text"])
                count += 1

    return total / count if count > 0 else 0

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python sentiment.py <filename>")
        sys.exit(1)

    avg = calc_score(sys.argv[1])
    print(f"Average sentiment score for {sys.argv[1]}: {avg}")

"""### Create a job script and run it, follow the script below
**Notice**: here the solution is provided to you, assuming that you wrote the python code and saved it correctly. Use a similar script for question 4 as well.

```bash

### 2. Copy your .sh that runs sentiment.py on all 5 files in parallel to here:
# File format should be:

#!/bin/bash

[Here add #SBATCH running configurations]
#SBATCH --job-name=sentiment_analysis            # Job name
#SBATCH --output=logs/sentiment_analysis_%a.out     # Out file
#SBATCH --error=logs/sentiment_analysis_%a.err      # Err file
#SBATCH --array=1-5                              # Run 5 tasks in parallel
#SBATCH --cpus-per-task=1


[Here write script that loops on the files and sends a job for each one]
cd /sci/labs/arieljaffe/dan.abergel1
INDEX=${SLURM_ARRAY_TASK_ID}
echo "Processing rating_${INDEX}.json"
#Runs the script
python -u sentiment.py rating_${INDEX}.json

---

### 3. Add the command used to run the script here (short, one line)0

---
sbatch run_q4.sh

### 4. Add Question 4 SLURM log output here:

---

[Make sure the output includes the average score for each rating file]

---
(env_torch) dan.abergel1@moriah-gw-02:/sci/labs/arieljaffe/dan.abergel1/scripts $ cat logs/sentiment_analysis_1.out
Processing rating_1.json
Average sentiment score for rating_1.json: -0.07307056555626844
(env_torch) dan.abergel1@moriah-gw-02:/sci/labs/arieljaffe/dan.abergel1/scripts $ cat logs/sentiment_analysis_2.out
Processing rating_2.json
Average sentiment score for rating_2.json: 0.13105751544592747
(env_torch) dan.abergel1@moriah-gw-02:/sci/labs/arieljaffe/dan.abergel1/scripts $ cat logs/sentiment_analysis_3.out
Processing rating_3.json
Average sentiment score for rating_3.json: 0.20521464424375077
(env_torch) dan.abergel1@moriah-gw-02:/sci/labs/arieljaffe/dan.abergel1/scripts $ cat logs/sentiment_analysis_4.out
Processing rating_4.json
Average sentiment score for rating_4.json: 0.3940137890037649
(env_torch) dan.abergel1@moriah-gw-02:/sci/labs/arieljaffe/dan.abergel1/scripts $ cat logs/sentiment_analysis_5.out
Processing rating_5.json
Average sentiment score for rating_5.json: 0.5438793767072085

### 5. Explanation:

[Explain your results here. Include:
- How do your script and python program work together?
- Do the average scores align with the ratings as you would expect? (e.g., do higher ratings have higher sentiment scores?)]

---

### 5. Explanation:

In this question, I created a SLURM job script (`run_q4.sh`) that processes all five rating files
(`rating_1.json` to `rating_5.json`) using a SLURM array. Each task in the array receives an index
(from 1 to 5) through the environment variable `SLURM_ARRAY_TASK_ID`. This index is then used to run
`sentiment.py` on the corresponding JSON file.

The important idea is that SLURM automatically launches the five tasks in parallel. This means that
each task runs its own independent instance of `sentiment.py`, and each task writes its output to a
separate log file using the `%a` placeholder in the `#SBATCH --output` directive. Because SLURM
handles each task separately, the `wait` command is not needed here (unlike in the version where
background processes are launched manually within a single job).

The Python script `sentiment.py` reads the reviews in each JSON file, computes a sentiment score for
every review, and prints the average sentiment score for the entire file. The outputs confirm that the
script ran correctly for all five files, and that each task produced independent results.

The sentiment scores align with intuition: files with higher ratings tend to have more positive
sentiment. For example, `rating_1.json` has a negative average sentiment (-0.073), while files with
higher ratings have progressively higher sentiment scores (e.g., 0.39 for rating 4 and 0.54 for
rating 5). This matches the expected relationship between star ratings and the positivity of the
review text, and shows that the sentiment calculation correctly captures this trend.

**Notes:**
- Use the `wait` command to ensure that your main script waits for all 5 processes to complete before finishing
- Make sure your `sentiment.py` prints output in a clear format, e.g., `Rating 1: Average score = X.XX`

## Q5: Comparing Sentiment Intensity of Extreme Reviews (6 points)

In Q4, you calculated sentiment scores. Now, let's investigate the *intensity* of sentiment in extreme reviews. We want to test whether 1-star reviews are expressed with the same emotional intensity as 5-star reviews.

Define "intensity" as the **absolute value** of the sentiment score:

**Intensity = |(count of positive words) - (count of negative words)|**

**Your Task:**

1.  Write a python script that calculates:
    *   The **mean intensity**.
    *   The **sample variance** of the intensity scores.
2. A SLURM job script `run_q5.sbatch` that runs the code above on both `rating_1.json` and `rating_5.json` in parallel as shown to you above for the 5 different files.
3. submit as done above
4. write another script hypothesis.py which performs a statistical test to evaluate the following hypothesis:
    *   **Null Hypothesis (H₀):** The mean intensity of 1-star reviews is equal to the mean intensity of 5-star reviews.
    *   **Alternative Hypothesis (H₁):** The mean intensities are different.
5. a slurm jub script to run this code and submit it
6. presnt the results and explain their meaning, do we accept or reject the null hypothesis

**Submission format:** just as in question 4


*Hint: Remember the formula for sample variance: s² = Σ(xᵢ - μ)² / (n - 1).*
"""

## your python code saved as run_q5.py here

"""import sys
import os
import json
import numpy as np

POSITIVE_PATH = "positive_words.txt"
NEGATIVE_PATH = "negative_words.txt"

# Load lexicons, lowercase, strip newlines
with open(POSITIVE_PATH) as f:
    pos = set(word.strip().lower() for word in f)
with open(NEGATIVE_PATH) as f:
    neg = set(word.strip().lower() for word in f)

def calculate_sentiment_intensity(text):
    if not text:
      return 0
    score = 0
    for word in text.split():
        word = word.lower()
        if word in pos:
            score += 1
        if word in neg:
            score -= 1
    return abs(score)

def calc_score(file_path):
    assert os.path.isfile(file_path), "Your path does not exist!"

    scores = []

    # NDJSON → one JSON object per line
    with open(file_path) as f:
        for line in f:
            rating = json.loads(line)
            scores.append(calculate_sentiment_intensity(rating["text"]))
    mean = np.mean(scores)
    var = np.var(scores,ddof=1)
    print(f"Mean of sentiment intensity = {mean} , Variance of sentiment intensity = {var}")

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python sentiment.py <filename>")
        sys.exit(1)

    calc_score(sys.argv[1])





**Output and Explanations Here**
```
Mean of sentiment intensity = 0.313 , Variance of sentiment intensity = 0.364



```

**Your slurm code and the code used for submitting it here:**
```
#!/bin/bash
#SBATCH --job-name=q5_intensity
#SBATCH --output=logs/q5_intensity.out
#SBATCH --error=logs/q5_intensity.err
#SBATCH --ntasks=2
#SBATCH --time=00:10:00
#SBATCH --mem=1G

module load python

srun -N1 -n1 python3 q5_intensity.py rating_1.json &
srun -N1 -n1 python3 q5_intensity.py rating_5.json &

wait

```

**Output and Explanations Here**
```
Sentiment intensity analysis for file rating_1.json
Mean of sentiment intensity = 0.313 , Variance of sentiment intensity = 0.364
Sentiment intensity analysis for file rating_5.json
Mean of sentiment intensity = 0.549 , Variance of sentiment intensity = 0.687
```
"""

## your python code hypothesis.py here

**Your slurm code and the code used for submitting it here:**

***** SBATCH FILE ********
```
#!/bin/bash
#SBATCH --job-name=hypothesis
#SBATCH --output=logs/hypothesis.out
#SBATCH --error=logs/hypothesis.err
#SBATCH --ntasks=1
#SBATCH --time=00:05:00
#SBATCH --mem=1G

srun python3 hypothesis.py
```




***** PYTHON CODE ********
```
import json
import numpy as np
from scipy.stats import ttest_ind

POSITIVE_PATH = "positive_words.txt"
NEGATIVE_PATH = "negative_words.txt"

with open(POSITIVE_PATH) as f:
    pos = set(w.strip().lower() for w in f)
with open(NEGATIVE_PATH) as f:
    neg = set(w.strip().lower() for w in f)

def calculate_sentiment_intensity(text):
    if not text:
        return 0
    score = 0
    for word in text.split():
        w = word.lower()
        if w in pos:
            score += 1
        if w in neg:
            score -= 1
    return abs(score)

def load_scores(path):
    scores = []
    with open(path) as f:
        for line in f:
            row = json.loads(line)
            text = row.get("text", "")
            scores.append(calculate_sentiment_intensity(text))
    return scores

def main():
    s1 = load_scores("rating_1.json")
    s5 = load_scores("rating_5.json")

    mean1 = np.mean(s1)
    mean5 = np.mean(s5)
    var1 = np.var(s1, ddof=1)
    var5 = np.var(s5, ddof=1)

    t, p = ttest_ind(s1, s5, equal_var=False)

    print("Mean intensity (1★):", mean1)
    print("Mean intensity (5★):", mean5)
    print("Sample variance (1★):", var1)
    print("Sample variance (5★):", var5)
    print("t-statistic:", t)
    print("p-value:", p)

    if p < 0.05:
        print("Reject H0 — intensities differ significantly.")
    else:
        print("Fail to reject H0 — intensities are similar.")

if __name__ == "__main__":
    main()
```


**Output and Explanations Here**
```
Mean intensity (1★): 0.3132299274073446
Mean intensity (5★): 0.5491556261121723
Sample variance (1★): 0.3643219054586051
Sample variance (5★): 0.6871453965789293
t-statistic: -297.13251851555765
p-value: 0.0
Reject H0 — intensities differ significantly.

```

"""# **Networks – 30 points**


We will work with the English Wikipedia hyperlink graph from March 2002.

**Dataset:** The file `enwiki_wikilink_graph_2002-03-01_csv.gz` contains the directed graph of hyperlinks between English Wikipedia articles from March 1, 2002. This is an early snapshot of Wikipedia when it was much smaller than today. You can download the file from:
https://zenodo.org/records/2539424

**Format:** The file is a gzipped CSV with tab-separated columns:
- `page_id_from`: ID of the source Wikipedia page
- `page_title_from`: Title of the source page
- `page_id_to`: ID of the target Wikipedia page  
- `page_title_to`: Title of the target page

Each row represents a directed edge (hyperlink) from one Wikipedia article to another.

## Q1: Exploratory Data Analysis (7 points)

Download and analyze the Wikipedia network dataset called `enwiki.wikilink_graph.2002-03-01.csv.gz` from https://zenodo.org/records/2539424

1. Load the dataset and display the first 5 non-comment lines. Explain what is shown and what the data represents (what each column means and how it defines a directed edge).

2. Count and report:
   - the number of nodes $n$
   - the number of edges $m$

3. Show two separate histograms:
   - one for the distribution of in-degrees
   - one for the distribution of out-degrees
   
   Use log scale on the axes if needed. Describe your results briefly (e.g., is the distribution heavy-tailed? symmetric? etc.).

4. Compute the fraction of dangling nodes (nodes with out-degree 0) and comment on why they are important for PageRank.

5. **Top Nodes:** Find and display the top 5 nodes by in-degree and top 5 nodes by out-degree. Show their full page titles.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
from scipy import sparse
from scipy.sparse import csr_matrix
import gzip
import os

# connect to your drive
from google.colab import drive
drive.mount('/content/drive')

#load data
# example: zipfile inside the "midterm" folder
os.chdir('/content/drive/MyDrive/midterm')

# Complete

# Load and display first 5 lines
file_name = "wikilink_graph.2002-03-01.csv"
with open(file_name) as f:
  lines = f.readlines()
header = lines[0].strip().split("\t")
rows = [line.strip().split("\t") for line in lines[1:]]
df = pd.DataFrame(data=rows, columns=header)
print(df[:5])

# Count nodes and edges
pages_id_to_list = list(df['page_id_from'])
pages_id_from_list = list(df['page_id_to'])
edges_set = set()
print(f"Num nodes = {len(set(pages_id_to_list + pages_id_from_list))}")

print(f"Num edges = {len(df)}")
# Create degree distribution histograms
# Degree distributions
out_deg = df['page_id_from'].value_counts()
in_deg  = df['page_id_to'].value_counts()

plt.figure(figsize=(14,5))

# Out-degree histogram
plt.subplot(1,2,1)
plt.hist(out_deg, bins=50, log=True)
plt.title("Out-degree distribution")
plt.xlabel("Out-degree")
plt.ylabel("Frequency (log scale)")

# In-degree histogram
plt.subplot(1,2,2)
plt.hist(in_deg, bins=50, log=True)
plt.title("In-degree distribution")
plt.xlabel("In-degree")
plt.ylabel("Frequency (log scale)")

plt.tight_layout()
plt.show()
# Compute dangling nodes fraction
dangling = round(len(set(df.page_id_to) - set(df.page_id_from)) / len(set(df.page_id_to)),3)
print(f"Dangling nodes fraction: {dangling}")

"""Dangling nodes — pages with out-degree 0 — play a crucial role in the PageRank algorithm.
Because they have no outgoing links, a random surfer who lands on such a page would get stuck and stop navigating.
If the algorithm did not handle dangling nodes explicitly, all PageRank mass would leak out of the system and the ranks would collapse to zero.
"""

# Convert IDs to titles using a mapping
id_to_title = dict(zip(df['page_id_from'], df['page_title_from']))
# Note: Some IDs may appear only in page_id_to; fill missing titles if necessary
id_to_title.update(dict(zip(df['page_id_to'], df['page_title_to'])))

# Top 5 by in-degree
top_in = in_deg.head(5)
print("Top 5 nodes by IN-degree:")
for node_id, deg in top_in.items():
    print(f"{node_id} ({id_to_title.get(node_id, 'Unknown')}) — in-degree = {deg}")

print("\n")

# Top 5 by out-degree
top_out = out_deg.head(5)
print("Top 5 nodes by OUT-degree:")
for node_id, deg in top_out.items():
    print(f"{node_id} ({id_to_title.get(node_id, 'Unknown')}) — out-degree = {deg}")

"""## Q2: PageRank Analysis (8 points)

Run the PageRank algorithm with damping factor $\beta = 0.9$ (teleportation probability $1-\beta = 0.1$) and convergence tolerance $10^{-6}$.

1. Initialize the PageRank vector uniformly and run the power iteration
   $$
   r^{(t+1)} = \beta P r^{(t)} + (1-\beta)\frac{\mathbf{1}}{n},
   $$
   where $P$ is the column-stochastic transition matrix after handling dangling nodes.  
   At each iteration, record and print the L1-difference $\|r^{(t+1)} - r^{(t)}\|_1$.  
   Plot this difference as a function of the iteration number and report how many iterations are needed for convergence.

2. Display the top 10 pages by PageRank score (show page titles and scores).

3. For each node $i$, compute
   - in-degree $\deg^{\text{in}}(i)$  
   - out-degree $\deg^{\text{out}}(i)$  
   - PageRank score $\text{PR}(i)$  

   Then:
   - Produce two scatter plots (log–log if needed):  
     (a) in-degree vs. PageRank, (b) out-degree vs. PageRank;  
   - For each of the two plots add a linear regression line, and also display (e.g. in the title) the correlation between the two variables ((a) in-degree and PageRank, (b) out-degree and PageRank);  
   - State which degree is more strongly correlated with PageRank and briefly comment whether this is surprising.

"""

import scipy
# Run the PageRank algorithm
pages = pd.concat([df['page_id_from'], df['page_id_to']]).unique()
pages.sort()
n = len(pages)

id2idx = {pid: i for i, pid in enumerate(pages)}
out_deg = df.groupby("page_id_from").size().reindex(pages, fill_value=0).values
P = np.zeros((n, n))
for _, row in df.iterrows():
    i = id2idx[row["page_id_from"]]   # from
    j = id2idx[row["page_id_to"]]     # to
    P[j, i] += 1

for i in range(n):
    if out_deg[i] == 0:
        P[:, i] = 1.0 / n
    else:
        P[:, i] /= out_deg[i]

beta = 0.9
tol = 1e-6

r = np.ones(n) / n

diffs = []
t = 0

while True:
    r_new = beta * (P @ r) + (1 - beta) * (1.0 / n)
    diff = np.sum(np.abs(r_new - r))
    diffs.append(diff)
    r = r_new
    t += 1
    if diff < tol:
        break

in_deg = df.groupby("page_id_to").size().reindex(pages, fill_value=0).values
out_deg = df.groupby("page_id_from").size().reindex(pages, fill_value=0).values
corr_in = scipy.stats.pearsonr(np.log1p(in_deg), np.log1p(r))[0]
corr_out = scipy.stats.pearsonr(np.log1p(out_deg), np.log1p(r))[0]

print("Converged after", t, "iterations.")

plt.figure(figsize=(6,4))
plt.plot(diffs)
plt.yscale("log")
plt.xlabel("Iteration")
plt.ylabel("L1 difference (log scale)")
plt.title("PageRank Convergence (β = 0.9)")
plt.show()

# Display top PageRank pages

top_idx = np.argsort(-r)[:10]
print("\nTop 10 pages by PageRank:")
for i in top_idx:
    print(f"PageID {pages[i]}  → PR = {r[i]:.6f}")

# Create scatter plots

plt.figure(figsize=(6,4))
plt.scatter(np.log1p(in_deg), np.log1p(r), s=5)
plt.xlabel("log(1 + in-degree)")
plt.ylabel("log(1 + PageRank)")
plt.title(f"In-degree vs PageRank (corr = {corr_in:.3f})")
plt.show()

plt.figure(figsize=(6,4))
plt.scatter(np.log1p(out_deg), np.log1p(r), s=5)
plt.xlabel("log(1 + out-degree)")
plt.ylabel("log(1 + PageRank)")
plt.title(f"Out-degree vs PageRank (corr = {corr_out:.3f})")
plt.show()

# Compute correlations

print("\nCorrelation in-degree vs PageRank:", corr_in)
print("Correlation out-degree vs PageRank:", corr_out)

"""**Explanation of correlation results:**

From the correlation analysis, the in-degree shows a much stronger correlation with PageRank (corr ≈ 0.53) compared to the out-degree (corr ≈ 0.25).
This result is not surprising: PageRank is fundamentally driven by incoming links, since a page becomes “important” when other important pages point to it.
In contrast, the number of outgoing links does not directly increase a page’s PageRank, which explains the weak correlation observed with out-degree.

[Solution text with Explanations of Computational Complexity of the different algorithms here]
"""

# Create random graph models

# Compare degree distributions

# Compute clustering coefficients

# Find strongly connected components

"""**Summary of findings:**

[Your analysis here]
In this analysis, I implemented the PageRank algorithm with damping factor β = 0.9 and a convergence tolerance of 10⁻⁶.
The algorithm converged after [XXX] iterations (your output will fill this number), and the L1-difference decreased monotonically, confirming numerical stability.

The top 10 pages by PageRank score reveal the most “important’’ nodes in the directed network. These are pages that receive a large amount of influence from other pages, not necessarily those with the highest out-degree.

To better understand what structural factors influence PageRank, I examined the relationship between:
	•	in-degree and PageRank;
	•	out-degree and PageRank.

The scatter plots show a clear positive association between in-degree and PageRank, and the correlation coefficient confirms this:
	•	corr(in-degree, PageRank) = [your value]
	•	corr(out-degree, PageRank) = [your value]

As expected, PageRank is much more strongly correlated with in-degree than with out-degree.
This is not surprising: PageRank fundamentally measures how much “importance’’ flows into a page from other pages, so nodes that accumulate many incoming links—especially from already important nodes—naturally obtain higher PageRank.
In contrast, out-degree does not contribute directly to a node’s PageRank; having many outgoing links only determines how its own rank is distributed to others, not how much it receives.

Overall, the results confirm the theoretical intuition behind PageRank: a page becomes important when important pages point to it, which explains the strong dependency on in-degree and the much weaker effect of out-degree.

## Q4: Personalized PageRank – Structure and Complexity (15 points)
---
**Submission note:** You may write your answers directly in the designated Markdown cells below, *or* submit a separate PDF with your written solutions (clearly labeled by question/part).

---

Let $P$ be the $n\times n$ **stochastic** transition matrix of the random surfer model - that is $p_{ij}=1/c_i$ if there is a link from $i$ to $j$ and $p_{ij}=0$ otherwise, where $c_i$ is the out-degree of vertex $i$. Assume that the adjacency matrix is **sparse** with $s$ non-zero entries.  
Define  
$$
B := \beta P, \qquad C_v := (1-\beta)\mathbf{1_n}v^\top.
$$
and let  
$$
G_v = B + C_v,
$$
where $0<\beta<1$, $\mathbf{1_n}$ is the all-ones column vector, and $v$ is a **teleportation / personalization** distribution ($v_i\ge 0$, $\sum_{i=1}^n v_i=1$).

The matrix $(1-\beta)\mathbf{1_n}v^\top$ is rank one and encodes personalization via $v$.

---

### (a) Structure of $G_v$ (3 pts)


- Determine all the possibilities for the sparsity (number of non-zero entries) of $B$ and $G_v$ in terms of $n,s$.
- Suppose that $rank(P)=r$. Determine all the possbilities for the rank of $B$ and $G_v$ in terms of $n,r$.  


---

### (b) Power method for the first eigenvector (4 pts)
Suppose now that we use the power method to find the **leading right eigenvector** of $G_v$, i.e. a vector $u$ and a scalar $\lambda$ such that $\lambda u^T = u^T G_v$ where $|\lambda|$ is maximized.

1. Using the decomposition $G_v = B + C_v$, write one iteration  
   $$
   x^{(t+1)} = G_v x^{(t)}
   $$
   **without forming $G_v$ or $C_v$ explicitly**. Use:
   - a sparse matrix–vector product with $B$,  
   - vector operations involving $v$ and $\mathbf{1}$.

2. Determine the computational complexity of **one iteration** in terms of $n$ and $s$ (Big-O).  

---

### (c) First $k$ eigenvectors via sequential deflation (4 pts)
Assume that all eigenvalues of $G_v$ are distinct in their absolute values: $|\lambda_1| > |\lambda_2| > ... > |\lambda_n|$. <br>
We now want the first $k$ right eigenvectors of $G_v$.

Design an algorithm that finds all $k$ leading right eigenvectors of $G_v$.
   - Use psuedo-code to describe your algorithm  
   - You may call the step performing one iteration from the previous sub-question
   - The algorithm should work **without forming $G_v$ or $C_v$ explicitly**, and utilize the special properties of $B$ and $C_v$
   - Do not worry in this sub-question about tolerance - i.e. you may assume for finding each eigenvector that you run your iterative algorithm until two consecutive iterations are close enough, i.e. $||x^{(t+1)}-x^{(t)}||_2$ is small, and that at this point you've found a close-enough approximation for the solution $x^*$. (The exact error analysis in terms of number of iterations, number eigenvectors $k$ and properties of the matrix $G_v$ is quite complicated and isn't covered in this course).


---

### (d) Complexity of the full algorithm (4 pts)

Assume that in each stage the power method iterates $x^{(t)}$ satisfy  
$$
\|x^{(t)} - x^*\|_2 \le \frac{a}{t^2}
$$
for some constant $a>0$, and that our stopping rule is such that we stop at the first time when $\|x^{(t)} - x^*\|_2 \le \epsilon$. <br>
Assume also for simplicity that errors in estimating a certain eigenvector to not accumulate when using it to estimate subsequent eigenvectors.

1. Up to Big-O (ignore constants and logs), how many **iterations** $t$ are needed to reach accuracy $\epsilon$ for one run of the power method?  
2. Derive the total computational complexity of your alrogithm from the previous sub-question (all $k$ eigenvectors) in each of the following cases, as a function of $n, s, k, \epsilon$:

Give Big-O expressions and indicate clearly the dependence on $n, s, k, \epsilon$.

**Complete**

[Your theoretical analysis here. Use plain text and markdown/latex for mathematical symbols]

###(a) Sparsity

Let $P \in \mathbb{R}^{n \times n}$ be a sparse stochastic matrix with $s$ non-zero entries.
Multiplying by a scalar does not change sparsity, so

$$
\text{nnz}(B)=\text{nnz}(\beta P)=s.
$$

The matrix
$$
C_v = (1-\beta)\,\mathbf{1}_n v^\top
$$
is an outer product. If $v$ has $k$ non-zero entries, then

$$
\text{nnz}(C_v)=nk , \quad 1 \le k \le n.
$$

Therefore,

$$
\max\{s,n\} \le \text{nnz}(G_v) \le n^2.
$$

###(a) **Rank analysis**

Since $\beta \neq 0$, scaling does not change the rank of the matrix. Therefore:

$$
\operatorname{rank}(B)
=
\operatorname{rank}(\beta P)
=
r.
$$

The matrix

$$
C_v = (1-\beta)\,\mathbf{1}_n\, v^\top
$$

is a non-zero outer product, and hence

$$
\operatorname{rank}(C_v)=1.
$$

Using the standard rank inequalities:

$$
\bigl|\operatorname{rank}(B)-\operatorname{rank}(C_v)\bigr|
\;\le\;
\operatorname{rank}(G_v)
\;\le\;
\operatorname{rank}(B)+\operatorname{rank}(C_v),
$$

we obtain:

$$
r-1 \;\le\; \operatorname{rank}(G_v)
\;\le\; r+1,
\qquad
\operatorname{rank}(G_v)\le n.
$$

Thus, the rank of $G_v$ may remain equal to $r$, may increase to $r+1$, or (rarely) decrease to $r-1$.

### (b) Power method for the first eigenvector

We apply one iteration of the power method
$$
x^{(t+1)} = G_v x^{(t)},
\qquad
G_v = B + C_v,
$$
without forming $G_v$ or $C_v$ explicitly.

Because
$$
C_v = (1-\beta)\,\mathbf{1}_n\, v^\top,
$$
we have
$$
C_v x^{(t)}
= (1-\beta)\,\mathbf{1}_n\, (v^\top x^{(t)}).
$$

Thus one iteration becomes
$$
x^{(t+1)}
= B x^{(t)} + (1-\beta)\,(v^\top x^{(t)})\,\mathbf{1}_n.
$$

---

### Computational complexity

1. Sparse matrix–vector multiplication:
   $$
   B x^{(t)} \quad \text{costs} \quad O(s).
   $$

2. Dot product:
   $$
   v^\top x^{(t)} \quad \text{costs} \quad O(n).
   $$

3. Forming and adding the scaled all-ones vector:
   $$
   (1-\beta)(v^\top x^{(t)})\mathbf{1}_n \quad \text{costs} \quad O(n).
   $$

Therefore, one iteration of the power method costs
$$
O(s + n).
$$

### (c) First $k$ eigenvectors via sequential deflation

We assume that the eigenvalues of $G_v$ satisfy
$$
|\lambda_1| > |\lambda_2| > \dots > |\lambda_n|.
$$
We want to compute the first $k$ right eigenvectors.

We use the power method with **deflation**.  Let `PowerStep(x)` denote one
iteration from part (b):
$$
\text{PowerStep}(x) := Bx + (1-\beta)(v^\top x)\mathbf{1}_n.
$$

**Algorithm (sequential deflation)**

Input: $B$, $v$, $\mathbf{1}_n$, integer $k$  
Output: approximate leading right eigenvectors $u_1,\dots,u_k$

1. Initialize an empty list of eigenvectors: $\mathcal{U} = \emptyset$.
2. For $j = 1,2,\dots,k$:
   1. Choose a random initial vector $x^{(0)} \in \mathbb{R}^n$.
   2. **Repeat until convergence** (e.g. $\|x^{(t+1)} - x^{(t)}\|_2$ is small):
      1. Perform one power-method step:
         $$
         y = \text{PowerStep}(x^{(t)})
         = Bx^{(t)} + (1-\beta)(v^\top x^{(t)})\mathbf{1}_n.
         $$
      2. **Deflation / orthogonalization**:
         for each previously computed eigenvector $u_i \in \mathcal{U}$,
         $i = 1,\dots,j-1$, set
         $$
         y \leftarrow y - (u_i^\top y)\,u_i.
         $$
      3. Normalize:
         $$
         x^{(t+1)} = \frac{y}{\|y\|_2}.
         $$
   3. When the iteration has converged, define $u_j = x^{(t+1)}$ and add
$u_j$ to the set $\mathcal{U}$.

This algorithm never forms $G_v$ or $C_v$ explicitly: it only uses the
sparse matrix–vector product with $B$ and vector operations involving $v$
and $\mathbf{1}_n$, as in part (b).

### (d) Complexity of the full algorithm

We are given that the power method satisfies the error bound  
$\|x^{(t)} - x^*\|_2 \le a / t^2$ for some constant $a > 0$, and we stop as soon as  
$\|x^{(t)} - x^*\|_2 \le \epsilon$.

#### (1) Number of iterations

We require $a / t^2 \le \epsilon$, hence  
$t^2 \ge a / \epsilon$ and therefore $t \ge \sqrt{a / \epsilon}$.

Up to Big-O (ignoring constants and logs), the number of iterations needed is
$ t = O(1 / \sqrt{\epsilon})$.

#### (2) Total complexity for all $k$ eigenvectors

From part (b), one iteration of the power method costs $O(s + n)$,
since it requires a sparse matrix–vector product with $B$ (cost $O(s)$)
and vector operations with $v$ and the all-ones vector (cost $O(n)$).

For a single eigenvector, the total cost is therefore
$O\big((s + n) / \sqrt{\epsilon}\big)$.

Repeating this for the first $k$ eigenvectors (and assuming that errors
do not accumulate between stages, as stated in the question), the total
computational complexity is
$O\big(k (s + n) / \sqrt{\epsilon}\big)$,
which shows explicitly the dependence on $n, s, k,$ and $\epsilon$.
"""

